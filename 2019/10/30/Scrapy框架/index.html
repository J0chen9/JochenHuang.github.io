<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Scrapy框架入门 | RhubarbC的博客</title><meta name="description" content="介绍Scrapy框架一些的基础知识和学习心得"><meta name="keywords" content="python爬虫,scrapy"><meta name="author" content="Rhubarb C"><meta name="copyright" content="Rhubarb C"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yoursite.com/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Scrapy框架入门"><meta name="twitter:description" content="介绍Scrapy框架一些的基础知识和学习心得"><meta name="twitter:image" content="http://yoursite.com/img/scrapy入门_cover.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy框架入门"><meta property="og:url" content="http://yoursite.com/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/"><meta property="og:site_name" content="RhubarbC的博客"><meta property="og:description" content="介绍Scrapy框架一些的基础知识和学习心得"><meta property="og:image" content="http://yoursite.com/img/scrapy入门_cover.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="next" title="python爬虫之如何实现并发" href="http://yoursite.com/2019/10/25/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://deehuang.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: {"languages":{"author":"作者: Rhubarb C","link":"链接: http://yoursite.com/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/","source":"来源: RhubarbC的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  copy_copyright_js: true
  
}</script></head><body><canvas class="fireworks"></canvas><div id="header"> <div id="page-header"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">RhubarbC的博客</a></span><i class="fa fa-bars fa-fw toggle-menu pull-right close" aria-hidden="true"></i><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lozad avatar_img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">4</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">7</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy介绍"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">scrapy介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy安装"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">scrapy安装</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy基础"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">scrapy基础</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#scrapy基本组件"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">scrapy基本组件</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy项目结构及Spider应用简介"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">scrapy项目结构及Spider应用简介</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#项目结构"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text">项目结构</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#基本命令"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text">基本命令</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Scrapy数据解析及处理"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">Scrapy数据解析及处理</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#初步数据解析处理"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">初步数据解析处理</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#选择器"><span class="toc_mobile_items-number">5.1.1.</span> <span class="toc_mobile_items-text">选择器</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#递归深度爬取"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text">递归深度爬取</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据格式化处理"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text">数据格式化处理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据持久化处理"><span class="toc_mobile_items-number">5.4.</span> <span class="toc_mobile_items-text">数据持久化处理</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Scrapy常用功能补充及拓展"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">Scrapy常用功能补充及拓展</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#URL去重"><span class="toc_mobile_items-number">6.1.</span> <span class="toc_mobile_items-text">URL去重</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#REPDupefilter类"><span class="toc_mobile_items-number">6.1.1.</span> <span class="toc_mobile_items-text">REPDupefilter类</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#拓展：自定制URL去重类"><span class="toc_mobile_items-number">6.1.2.</span> <span class="toc_mobile_items-text">拓展：自定制URL去重类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#null"><span class="toc_mobile_items-number">6.2.</span> <span class="toc_mobile_items-text"></span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#结语"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">结语</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy介绍"><span class="toc-number">1.</span> <span class="toc-text">scrapy介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy安装"><span class="toc-number">2.</span> <span class="toc-text">scrapy安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy基础"><span class="toc-number">3.</span> <span class="toc-text">scrapy基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#scrapy基本组件"><span class="toc-number">3.1.</span> <span class="toc-text">scrapy基本组件</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy项目结构及Spider应用简介"><span class="toc-number">4.</span> <span class="toc-text">scrapy项目结构及Spider应用简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#项目结构"><span class="toc-number">4.1.</span> <span class="toc-text">项目结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基本命令"><span class="toc-number">4.2.</span> <span class="toc-text">基本命令</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy数据解析及处理"><span class="toc-number">5.</span> <span class="toc-text">Scrapy数据解析及处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#初步数据解析处理"><span class="toc-number">5.1.</span> <span class="toc-text">初步数据解析处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#选择器"><span class="toc-number">5.1.1.</span> <span class="toc-text">选择器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#递归深度爬取"><span class="toc-number">5.2.</span> <span class="toc-text">递归深度爬取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据格式化处理"><span class="toc-number">5.3.</span> <span class="toc-text">数据格式化处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据持久化处理"><span class="toc-number">5.4.</span> <span class="toc-text">数据持久化处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy常用功能补充及拓展"><span class="toc-number">6.</span> <span class="toc-text">Scrapy常用功能补充及拓展</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#URL去重"><span class="toc-number">6.1.</span> <span class="toc-text">URL去重</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#REPDupefilter类"><span class="toc-number">6.1.1.</span> <span class="toc-text">REPDupefilter类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#拓展：自定制URL去重类"><span class="toc-number">6.1.2.</span> <span class="toc-text">拓展：自定制URL去重类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#null"><span class="toc-number">6.2.</span> <span class="toc-text"></span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#结语"><span class="toc-number">7.</span> <span class="toc-text">结语</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/scrapy入门topimg.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">Scrapy框架入门</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-10-30<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-10-31</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">python网络爬虫</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/Scrapy/">Scrapy</a></span><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">7.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 23 分钟</span><span class="post-meta__separator">|</span><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="scrapy介绍"><a href="#scrapy介绍" class="headerlink" title="scrapy介绍"></a>scrapy介绍</h1><p>python爬虫我们一般使用:<br>requests模块帮助我们发送请求；<br>使用异步IO模块（gevent、Twisted、Tornado、asyncio）帮助我们异步发送请求，提高并发！BeautifulSoup模块帮助我们解析数据，提取我们想要的东西</p>
<p>如果有一个需求，要把一个网站内“所有”的数据提取出来，即我要拿到这个网站内的所有的页面。我就需要用循环或者递归去找到这个网站下所有的url并且进入这个url再做此过程直到找到网页底部。（给一个网站首页我们是可以找到这个网站上所有的页面的）。我们如果通过上面的知识自己去找的话其实还是挺费劲的，特别是如果网页的<strong>深度</strong>很大的话，想想就折磨…….python作为强大的“面向调包”语言，有没有一个模块能帮我们实现这样的功能呢？答案当然是YES!</p>
<p>我们把上面说的例子所实现的功能叫做深度查找，而我们的scrapy框架就给我们提供了这样便捷的功能。</p>
<p>什么是scrapy框架？一般我们叫框架的那必然是集成了许多功能的大型模块。scrapy框架就是集成了我们所学的所有单点功能的整合，就是上面所说的所有模块的功能。不仅如此，它还提供了许多便捷强大的功能，如：深度查找。</p>
<hr>
<h1 id="scrapy安装"><a href="#scrapy安装" class="headerlink" title="scrapy安装"></a>scrapy安装</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Linux</span><br><span class="line">      pip3 install scrapy</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">Windows</span><br><span class="line">      a. pip3 install wheel</span><br><span class="line">      b. 下载twisted http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</span><br><span class="line">      c. 进入下载目录，执行 pip3 install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl</span><br><span class="line">      d. pip3 install scrapy</span><br><span class="line">      e. 下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/</span><br></pre></td></tr></table></figure>

<h1 id="scrapy基础"><a href="#scrapy基础" class="headerlink" title="scrapy基础"></a>scrapy基础</h1><h2 id="scrapy基本组件"><a href="#scrapy基本组件" class="headerlink" title="scrapy基本组件"></a>scrapy基本组件</h2><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。<br>其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。</p>
<p>有没有跟我一样好奇，scrapy内部是怎么做的？</p>
<p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下</p>
<p><img alt="scrapy架构图" data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/scrapy%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg" class="lozad"></p>
<p>图中描述的是Scrapy主要包括了以下组件：</p>
<ul>
<li><strong>引擎(Scrapy)</strong><br>用来处理整个系统的数据流处理, 触发事务(框架核心)；<br>让爬虫深度爬取页面的时候，怎么让爬虫知道自己下一步该走到哪里呢？这就需要引擎帮我们做控制，它主要控制的就是写循环或递归让爬虫能一直深度的执行；引擎还帮我们做很多事情，例如调度任务，引擎去调度器把URL拿过来，然后让虫子去这个URL执行，接着引擎会去调度下载器把页面内容下载下来，虫子从中解析提取自己需要的信息….这些过程都是需要引擎去参与调度的！</li>
<li><strong>调度器(Scheduler)</strong><br>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
<li><strong>下载器(Downloader)</strong><br>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
<li><strong>爬虫(Spiders)</strong><br>爬虫是<strong>主要干活的</strong>, 用于从特定的网页中<strong>提取</strong>自己需要的信息, 即所谓的实体(Item)。<u>用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</u></li>
<li><strong>项目管道(Pipeline)</strong><br>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li><strong>下载器中间件(Downloader Middlewares)</strong><br>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li><strong>爬虫中间件(Spider Middlewares)</strong><br>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li><strong>调度中间件(Scheduler Middewares)</strong><br>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>
<p>可以简单的理解为：<br>调度器（Scheduler）就是一个队列，用于存储URL；<br>引擎（Scrapy）可以认为是一个while循环，不断的从调度器队列<strong>取数据</strong>（URL）然后调度下载器去下载页面让爬虫（Spiders）进行解析提取数据（实体）操作。<br>爬虫（Spiders）主要就是拿到网页内容做数据的解析和提取，提取出来的信息我们就叫实体（Item）<br>项目管道（ItemPipeline）负责处理爬虫爬取到的实体（虫子拿到的数据就丢给他了），进行一些操作，比如放入数据库，保存本地硬盘(这个就叫持久化操作)、对验证进行验证，还有再过滤实体中的一些信息等等<br>总结一下就是：<br>每个爬虫（Spider）都有一个初始的URL（一开始我们自己就要定义好）</p>
<ol>
<li>引擎（scrapy）第一步是把所有的Spider的初始URL放到调度器（Scheduler）中（可以记为引擎是梦开始的地方）</li>
<li>引擎再去用调度器中取URL然后调我们的下载器去下载页面内容</li>
<li>下载器把下载的结果给到我们的爬虫（Spiders）</li>
<li>爬虫拿到下载的结果后接下来要执行什么操作就由我们自己决定了，如果我们想要让某个URL继续操作就把它继续放到调度器就行了（深度爬取）；如果我们要把数据进行持久化（放到数据库）就丢到Pilpeline去</li>
</ol>
<p>实际的框架的执行流程是这样的：</p>
<ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器</li>
<li>下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>
<li>解析出的是链接（URL）,则把URL交给调度器等待抓取</li>
</ol>
<p>整个环节都是通过<strong>引擎</strong>去运行的，所以说引擎是scrapy框架的核心！</p>
<p>实际对框架的使用中，我们就是要对Spider做操作，要做的就两点：</p>
<ol>
<li>第一是指定URL，第二就是解析返回值的内容</li>
<li>解析响应内容<ul>
<li>给调度器，继续进行爬取操作</li>
<li>给item pipeline用于做格式化（item用来做格式化）持久化(pipeline用来做持久化)</li>
</ul>
</li>
</ol>
<h1 id="scrapy项目结构及Spider应用简介"><a href="#scrapy项目结构及Spider应用简介" class="headerlink" title="scrapy项目结构及Spider应用简介"></a>scrapy项目结构及Spider应用简介</h1><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><p>创建一个scrapy工程项目<br><code>scrapy startproject project_name</code>  </p>
<p>然后我们的<em>project_name</em>工程目录下就有了这些东西<br><img alt="scrapy工程目录" data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/scrapy%E5%B7%A5%E7%A8%8B%E7%9B%AE%E5%BD%95.jpg" class="lozad"></p>
<p>文件说明：</p>
<ul>
<li>scrapy.cfg  项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）</li>
<li>items.py   设置数据存储模板，用于结构化数据，如：Django的Model</li>
<li>pipelines   数据处理行为，如：一般结构化的数据持久化</li>
<li>settings.py 配置文件，如：递归的层数、并发数，延迟下载等</li>
<li>spiders    爬虫目录，如：创建文件，编写爬虫规则</li>
</ul>
<p>点进spiders文件发现里面只有一些初始化文件，没有东西（因为我们还没有创建蜘蛛）</p>
<p>此时我们进入该工程目录下，在命令行执行<code>scrapy genspider chouti chouti.com</code>就创建了一个蜘蛛并且给他初始化了URL，此时spiders文件下就多出来了我们创建的蜘蛛<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/1.jpg" class="lozad"></p>
<p>点开这个文件发现，它创建了一个类。类中：<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/2.jpg" class="lozad"></p>
<ul>
<li><code>name</code>属性是我们创建蜘蛛时写的第一个参数，表示爬虫的名字</li>
<li><code>allowed_domins</code>属性是我们创建蜘蛛写的第二个参数，表示支持的域名，可以理解为爬虫给的边界，如果网页内部有别的域名的链接，它会去这里面比较，然后决定爬不爬！（注意的是初始URL不受这个边界限制，因为程序一执行就发过去了，在递归爬取网页里的其他url的时候才会去比较！）</li>
<li><code>start_urls</code>属性默认是基于域名生成的URL，我们可以自己改动这个URL，因为这就是我们要爬取的起始URL</li>
<li><code>parse</code>方法的作用是<u>访问起始URL并获取结果后的回调函数</u>。传入的参数response,就是下载器下载网页的给爬虫返回值对象，它封装了许多东西：<ul>
<li><code>response.url</code>能拿到我们的请求的url，</li>
<li><code>response.text</code>能拿到我们请求的内容</li>
<li><code>response.body</code>能拿到请响应的所有东西（包括请求头）</li>
<li><code>response.meta</code>这个玩意返回的是一个字典{}，包含了很多东西，比如当前深度depth,我们记住{‘depth’:’深度’}就完事</li>
</ul>
</li>
</ul>
<p>定义好起始URL和回调函数后，我们就写好了我们的爬虫了，接下来就可以去执行爬取网站。调用的命令行是<code>scrapy crawl 爬虫名</code>。如果不像看到它的执行日志的话，可以使用<code>scrapy crawl 爬虫名 --nolog</code></p>
<h2 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h2><p>总结一下使用创建scrapy并简单实用爬虫应用的操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy startproject 工程名</span><br><span class="line">	- 在当前目录中创建中创建一个项目文件（类似于Django）</span><br><span class="line">	</span><br><span class="line">cd 工程名</span><br><span class="line"></span><br><span class="line">scrapy genspider 爬虫名 支持爬取的域名</span><br><span class="line">	- 创建爬虫应用</span><br><span class="line">	</span><br><span class="line">#进入spiders目录，打开爬虫名.py进行编辑</span><br><span class="line"></span><br><span class="line">scrapy crawl 爬虫名 --nolog</span><br><span class="line">	- 运行单独爬虫应用</span><br><span class="line">	</span><br><span class="line">scrapy list</span><br><span class="line">	- 展示爬虫应用列表</span><br></pre></td></tr></table></figure>

<p> <em>注意：一般创建爬虫文件时，以网站域名命名</em> </p>
<p>PS: window中去查看<code>response.text</code>时候可能会因为编码问题而无法显示内容，我们需要去改名一下编码内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> sys,os,io</span><br><span class="line">sys.stdout=io.TextIOWrapper(sys.stdout.buffer,encoding=<span class="string">'gb18030'</span>) <span class="comment"># 标准输入输出</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChoutiSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">	name = <span class="string">'chouti'</span></span><br><span class="line">	allowed_domains = [<span class="string">'chouti.com'</span>]</span><br><span class="line">	start_urls = [<span class="string">'http://dig.chouti.com/'</span>]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">		print(response.url)</span><br><span class="line">        print(response.text)</span><br><span class="line">		content=str(response.body,encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">		print(content)</span><br></pre></td></tr></table></figure>

<p>这就完成了我们爬取功能！</p>
<p>爬虫标准的三步走：下载网页—&gt;解析提取数据—&gt;保存数据(放到数据库去)</p>
<p>接下来我们就来讲一下怎么使用scrapy解析数据</p>
<h1 id="Scrapy数据解析及处理"><a href="#Scrapy数据解析及处理" class="headerlink" title="Scrapy数据解析及处理"></a>Scrapy数据解析及处理</h1><p>在我们<code>parse</code>方法中，我们拿到下载器下载后返回给我们的页面数据，在这里我们可以做一些简单的初步数据处理。然后再决定接下来要做什么处理操作：进一步爬取（实体（提取后的信息）是url）；格式化初步处理的数据然后再进一步去处理这些数据或者是数据持久化（放进数据库）<br>但是所有的操作都需要基于我们初步的数据解析才能往下操作。</p>
<h2 id="初步数据解析处理"><a href="#初步数据解析处理" class="headerlink" title="初步数据解析处理"></a>初步数据解析处理</h2><p>这里要介绍的是在<code>parse</code>中我们如何对数据的初步简单处理</p>
<h3 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h3><p>Scrapy使用xpath的选择器来对数据来过滤出我们想要的标签对象和内容</p>
<p>在Scrapy中，有<em>Selector<em>或</em>HtmlXpathSelector<em>用于结构化HTML代码（转换为标签对象）并提供选择器功能，利用它可以*</em>初步*<em>的解析过滤数据（为什么是初步，因为在这里只是做了简单的数据处理，前面介绍scrapy主要组件的时候又说道</em>Item</em> <em>Pipeline</em>组件提供了数据持久化和格式化功能，通过Item统一格式化数据再传给pipeline也可以做更进一步的处理）。</p>
<p>啊?什么是选择器?接触过<em>css</em>和<em>jquery</em>就知道是按语法去匹配标签对象的一种方法！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> sys,os,io</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> selector,HtmlXpathSelector</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChoutiSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">	name = <span class="string">'chouti'</span></span><br><span class="line">	allowed_domains = [<span class="string">'chouti.com'</span>]</span><br><span class="line">	start_urls = [<span class="string">'http://dig.chouti.com/'</span>]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 标签对象列表，Selector就是把文档变成标签对</span></span><br><span class="line">		hxs=Selector(response=response).\\</span><br><span class="line">        xpath(<span class="string">'//div[@id"=1"]/div[class="item"]'</span>)\\  <span class="comment"># 选择器，用来找标签对象</span></span><br><span class="line">        .extract()  <span class="comment"># 拿到的是标签对象，如果想拿字符串内容就要使用extract方法</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hxs:</span><br><span class="line">            print(i)</span><br><span class="line">        <span class="comment">### 更多选择器的使用例子 ### </span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[2]')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[@id]') #找到所有有id的a标签</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[@id="i1"]')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[@href="link.html"][@id="i1"]')</span></span><br><span class="line"><span class="comment"># print(hxs) # 双条件的运用</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[contains(@href, "link")]')</span></span><br><span class="line"><span class="comment"># print(hxs) # 包含link就匹配查找，跟字符串里面的in操作是一样的</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[starts-with(@href, "link")]')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]/text()').extract()</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]/@href').extract()</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('/html/body/ul/li/a/@href').extract()</span></span><br><span class="line"><span class="comment"># print(hxs) # 不越级一个孩子一个孩子往下找</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//body/ul/li/a/@href').extract_first()</span></span><br><span class="line"><span class="comment"># print(hxs) # //直接越级在整个文档对象的子子孙孙去找</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># ul_list = Selector(response=response).xpath('//body/ul/li')</span></span><br><span class="line"><span class="comment"># for item in ul_list:</span></span><br><span class="line"><span class="comment">#     v = item.xpath('./a/span') # 在当前基础上往下去找</span></span><br><span class="line"><span class="comment">#     # 或</span></span><br><span class="line"><span class="comment">#     # v = item.xpath('a/span')</span></span><br><span class="line"><span class="comment">#     # 或</span></span><br><span class="line"><span class="comment">#     # v = item.xpath('*/a/span')</span></span><br><span class="line"><span class="comment">#     print(v)</span></span><br></pre></td></tr></table></figure>

<p>HtmlXpathSelector模块和Selector的使用方式是一样的，只不过前者的选择器使用的是<code>select(&#39;这里用xpath&#39;)</code>方法。</p>
<p>xpath选择器的基本使用：<br>Selecotor可以把我们的<strong>HTML文档变成标签对象</strong>，通过它下面的<code>xpath()</code>方法可以去选择我们想要的标签对象<u>列表列表列表</u>。<br>想要把<u>标签对象列表转换成字符串列表</u>，需要使用<code>extract()</code>方法</p>
<p><code>//</code>表示从整个Html对象子子孙孙去找标签，<code>//div</code> 就是去整个<em>html</em>文档去找<em>div</em>标签<br><code>.//</code>当前对象的子孙中去找<br><code>/</code>表示去找子标签  如 ./表示的是从当前标签的孩子里面去找<br><code>[@]</code>筛选条件，表示标签的属性  如<code>//div[@id=1]/div[class=&#39;item&#39;]</code> 表示在所有的标签中找到<em>id=1</em>的标签，然后再找到它下面<em>class</em>属性为<em>item</em>的标签</p>
<p>如果我们想从某个已经获取到的标签对象去找它子子孙孙对象需要用 <code>.//</code>去找，如：假设obj是我们已经通过Selector选择出来的某个div标签对象 ,通过<code>obj.xpath(.//a[@class=&#39;hidden-content&#39;])</code>就可以拿到该对象下面的所有属性<em>class=‘hidden-content’</em>的<strong>标签对象列表</strong></p>
<p>如果我们想要拿到对象列表中的某一个元素，可以通过索引的方法，如<code>//a[2]</code>，表示在找到Html中所有的a标签然后返回第二个a标签元素。</p>
<p>如果我们拿到标签内的文本内容，需要在xpath方法里面/text()。如：<code>obj.xpath(&quot;.//a[@class=&#39;hidden-content&#39;]/text()&quot;)</code>表示拿到该对象下面的所有属性<em>class=‘hidden-content’</em>的标签对象的文本列表。</p>
<p>如果想拿到字符串内容可以使用<code>extract()</code>方法。但是通过选择器筛选的结果都是标签对象列表。所以这个方法返回<strong>字符串内容的也是一个列表</strong>，因为我们是对对象列表进行的<em>extract</em>。如果我们知道该列表只有一个元素，即我们只需要取里面的第一个元素，可以使用<code>extract_first()</code>就可以把对象转换为字符串且只拿列表的第一个元素</p>
<p>小总结一下：</p>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/3.jpg" class="lozad"></p>
<p> 补充：<br>如果要拿标签对象中属性可以使用 <code>/@xxx</code>  如：<code>obj.xpath(&#39;//div[@id=&quot;dig_lcpage&quot;]/a/@href&#39;)</code>拿到<code>id=dig_lcage</code>的<code>div</code>标签对象下的所有a标签的<strong><em>href</em>属性值</strong><br><em>PS：<code>//a[@id]</code>和<code>//a/@id</code>是有区别的，前者是知道有所具有id属性的a标签。后者是找到所有a标签上的id的<u>值</u></em></p>
<p>如果我们有需求查询的是某个属性是的值是以什么什么开头的，xpath提供了这样的方法<code>starts-with(@属性名,&#39;属性值&#39;)</code>如：<code>Selector(response=response).xpath(&#39;//a[starts-with(@href,&quot;/all/hot/recent/&quot;)]&#39;/@href)</code>，这就表示在整个Html标签对象下找到<em>href</em><u>属性内容</u>是以<code>/all/hot/recent/</code>开头的所有<em>a</em>标签对象，并获取它们的<em>href</em>属性值(为了加深上面获取属性值的方法，所以这里例子复杂了点~)</p>
<p>选择器还支持正则表达式去模糊匹配要选择的对象！固定写法<code>re:text(&#39;属性名，&#39;模糊匹配的属性值&#39;)</code>如：<code>Selector(response=response).xpath(&#39;//a[re:text(@id,&quot;i\d+)]&quot;)</code></p>
<hr>
<p>这里大致的就完成了我们对数据的初步处理操作，接下来就是我们按需求对实体（提取出来的信息），是进一步过滤处理还是再进一步的深度爬取的处理操作呢？亦或者是把数据放入数据库中。</p>
<h2 id="递归深度爬取"><a href="#递归深度爬取" class="headerlink" title="递归深度爬取"></a>递归深度爬取</h2><p>首先要始终明确记住一点是：在scarpy组件重爬虫（Spider）的就是<strong>主要干活的</strong>, 用于从特定的网页中<strong>提取</strong>自己需要的信息, 即所谓的实体(Item)。<u>用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</u></p>
<p>如果我们的需求是爬取抽屉网站上里所有的页码，那我们上面提取的实体就是URL。回忆之前说的scrapy工作流程！我们只需要把URL再压入调度器，然后让下载器去下载，最后把下载内容返回给爬虫即可完成进一步的爬取工作。</p>
<p>因为我们的需求是找到所有的页码内容，所以我们每次的爬取的初步处理都是一样的：提取URL实体，所以就可以采用递归爬取(执行一样的处理函数)的方式去实现我们的需求。</p>
<p>怎么把任务丢到调动器呢？<br><code>from scrapy.http improt Request</code><br><code>yield Request(url=url)</code><br>完成！这样scrapy就会把这个Request对象添加到调度器里面了，并且下载器会自动去下载（这个中间过程是引擎做的）<br>Request是一个封装用户请求的类，在回调函数中yield该对象表示继续访问<br><strong>要注意的是：这里必须要<code>yield</code>一下，引擎看到<code>yield</code>就会自动的把Request放到调度器去了。</strong></p>
<p>但是有一个问题，下载的内容它会交给哪个爬虫去处理呢？它自己是不知道的，所以我们需要再指定下回调函数（可以自己自定义要执行的回调函数，但是我们这里需要使用深度递归查找的方法）。<br><code>Request(url=url,callback=self.parse)</code> <em>self.parse</em>就是指我这个爬虫的parse函数。这样下载器下载完后就调度一下self.parse方法。</p>
<p>所以记住！<code>yelid Request(url,callback)</code>表示的就是把任务压到调度器然后下载任务，下载完后调用我们的<em>callback</em>函数</p>
<p>这里容易有一个疑问，那么我们的初始start_url是怎么知道我们要去执行我们的<code>parse</code>函数的。于是我看了一下源码，它是这样实现的<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/4.jpg" class="lozad"><br>程序实行它会遍历我们的<em>start_urls</em>列表拿到起始url，然后把每个utl传入执行<code>make_requests_from_ur()</code>方法。再看看这个方法在做了什么<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/5.jpg" class="lozad"><br>发现，它也是创建了<em>Request</em>对象把url传进去（放入调度器并下载），这里的<code>callback</code>默认情况就是去执行我们的<code>self.parse</code>。</p>
<p>它其实就是做了这样一件事！<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/6.jpg" class="lozad"><br>通过它我们也可以重写parse方法，不一定要用parse。即自己指定最开始请求的方法<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/1572463413(1).jpg" class="lozad"></p>
<p>所以通过：<br><code>from scrapy.http improt Request</code><br><code>yeild Request(url=url,callback=self.parse)</code><br>我们就实现了我们的递归实现深度爬取了！</p>
<p>理解下深度爬取：因为页码一般都是动态显示的（我们首页可能就只能看到1-10的页面，当点到7后，页面11才会出现,10的时候出现14页）<em>PS:我们的爬虫会在指定的深度上自动再走一层！</em><br>当前爬取的页面我们就叫深度1。基于这个页面去点进去的所有页码就是深度2。<br>深度1能拿到的也就10个页码，但是会再走一层即拿到深度2的东西，所以爬到14个页码<br>深度2是从1点击去的页码，它最多点到10,所以只能拿到14个页码，但是它会再走一层，即拿到深度3的东西，所以爬到18个页码<br>深度3就是基于深度2拿到14个页码，所以能拿到18个页面，再走一层就是22页码 以此类推…….</p>
<p>PS:我们可以在 <em>settings.py</em>中设置<em>DEPTH_LIMIT = x</em>来指定“递归”的层数。<br>DEPTH_LIMIT默认是0表示的是无限深度。</p>
<p>总结：<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/7.jpg" class="lozad"></p>
<p>深度查找就是反复的不停的执行“一个”操作，直到找完了才停止！</p>
<h2 id="数据格式化处理"><a href="#数据格式化处理" class="headerlink" title="数据格式化处理"></a>数据格式化处理</h2><p><strong>scarpy的数据格式化是通过items完成的。</strong></p>
<p>在工程目录的items就是一个类，它默认是以我们的工程名命名，然后啥都没有做，如：<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/8.jpg" class="lozad"></p>
<p>我们说到上述<code>parse</code>中的实例只是简单的处理，所以在<code>parse</code>方法中直接处理。如果对于想要获取更多的数据处理，则可以利用Scrapy的items将数据格式化，然后统一交由pipelines来处理 。</p>
<p>也就是说在parse中处理完的数据就丢到item，它会帮我们自动的将数据格式化，做统一的管理</p>
<p>怎么格式化？就是把我们的传入的数据和它的变量做成一个字典，如：<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/11.jpg" class="lozad"></p>
<p>items就帮我们把处理的数据变成{title:obj,href:obj}这样的字典数据。</p>
<p>这样记住就行：我们在Parse筛选后的数据可以直接丢到item对象去，item就是来做一个临时的统一化的数据管理的！</p>
<h2 id="数据持久化处理"><a href="#数据持久化处理" class="headerlink" title="数据持久化处理"></a>数据持久化处理</h2><p><strong>scrapy的数据持久化是基于pipeline去做的</strong></p>
<p>Pipeline会从items拿到格式化的所有数据，然后进行数据持久化</p>
<p>使用<code>yield item_obj</code>scrapy就会自动的把item对象传递给pipeline</p>
<p>在工程目录的items就是一个类，它默认是以我们的工程名命名,它下面有一个<code>process_item()</code>方法：</p>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/9.jpg" class="lozad"></p>
<p>scrapy会把item对象和spider对象传给它</p>
<p>注意：要使用pipeline需要在settings里面注册, 可以注册多个pipelines,就是说在pipelines模块里面我们可以写多个pipeline类，只要在这里注册上就行了，但是它们是有执行顺序的。每行后面的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/10.jpg" class="lozad"></p>
<p>对数据做持久化处理（爬取图片的案例）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'xiaohua.txt'</span>, <span class="string">'w'</span>) </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        v = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        self.file.write(v)</span><br><span class="line">        self.file.write(<span class="string">'\n'</span>)</span><br><span class="line">        self.file.flush()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'imgs'</span>):</span><br><span class="line">            os.makedirs(<span class="string">'imgs'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        response = requests.get(item[<span class="string">'url'</span>], stream=<span class="literal">True</span>)</span><br><span class="line">        file_name = <span class="string">'%s_%s.jpg'</span> % (item[<span class="string">'name'</span>], item[<span class="string">'school'</span>])</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(<span class="string">'imgs'</span>, file_name), mode=<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>要明确一点的是：只要注册了Pipelines，那么所有的爬虫都会去执行这些pipelines。如果我们想针对每个爬虫去做不同的操作，我们可以对pipelines传入的spider做判断例如：<code>if spider.name == &#39;baidu&#39;</code>就行了，这也是它为什么会被传进来。</p>
<p>上面我们有说到parse只是做简单的处理，在pipeline我们也能对处理的数据再进一步的做处理。<strong>所以前面parse的处理我才叫初步处理</strong>。但是是否再做处理看个需求，所以<strong>初步</strong>这个词也是相对的，不要混乱…..</p>
<p>总结一下我们使用scarpy大致的流程:</p>
<ol>
<li>通过选择器采集我们想要的数据（要做数据再处理或持久化工作） —&gt;  把每个实体(提取出来的数据)分装到我们的<em>item</em>对象里面  —&gt; 传给Piperline(<code>yield item</code>对象)，让<em>Piperline</em>帮我们做处理 </li>
<li>通过选择器采集我们想要的数据（要做深度爬取工作）—&gt; 把实体（url）封装到<em>Request</em>对象中，并设置回调函数 <code>Request(url=url,callback=self.parse/other_fun)</code>—&gt;然后放到调度器去 (yield Request对象)，引擎自动帮我们调度并下载返回给爬虫执行回调</li>
</ol>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/12.jpg" class="lozad"></p>
<h1 id="Scrapy常用功能补充及拓展"><a href="#Scrapy常用功能补充及拓展" class="headerlink" title="Scrapy常用功能补充及拓展"></a>Scrapy常用功能补充及拓展</h1><h2 id="URL去重"><a href="#URL去重" class="headerlink" title="URL去重"></a>URL去重</h2><p>在进行深度爬取的时候，对爬取回来的URL去重是一个常见且重要的操作。Scrapy里面给我们提供方便的URL去重操作的功能。只要设置Request中的<code>Request（dont_filter=flase）</code>,就会帮我们自动去重（它默认就是false！即默认就会帮我们去重）</p>
<h3 id="REPDupefilter类"><a href="#REPDupefilter类" class="headerlink" title="REPDupefilter类"></a>REPDupefilter类</h3><p>单单设置一下dont_filter参数就能帮我们做去重的事情，这也太方便了！但是我十分好奇它是怎么实现的：<br>scrapy中的查看<code>Request</code>对象的参数，发现它默认会调用一个类去帮我们对传入URL做去重的事情：<br><code>from scrapy.dupefilter import RFPDupeFilter</code></p>
<p>看一下这类里面有什么<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/13.jpg" class="lozad"></p>
<p>图中的<code>request_seen</code>功能就是这个请求是否已经被查看过了，return False表示的是没有查看过，所以会丢到调度器去爬取，return True表示已经查看过了，在这里就会被过滤掉不会丢到调度器去（BaseDupeFilter这是一个基类，看样子是做归一化设计的）<br>而默认情况下对URL的判断去重处理是实际上通过这个类去做的<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/14.jpg" class="lozad"></p>
<p>在执行scrapy程序的时候会自动把它读出来，但是它默认的实现的URL去重功能是把URL放到文件里面去然后再对比文件里的URL是否已经存在，如果我们不想放在文件里去而是直接在缓存里对比或者是放到数据库去的话怎么办呢？</p>
<h3 id="拓展：自定制URL去重类"><a href="#拓展：自定制URL去重类" class="headerlink" title="拓展：自定制URL去重类"></a>拓展：自定制URL去重类</h3><p>上面的意思就是，默认给我提供的去重功能定义的比较死板！如果我们想灵活处理URL的存储方式，我们可以自己去定制这个类！去实现去重功能</p>
<p>通过观察源码，发现RFPDupeFilter类中方法的作用是这样的（执行顺序也如下面所示）：</p>
<ul>
<li>通过<code>from_settings</code>来创建对象</li>
<li><code>__init__</code>进行对象初始化</li>
<li><code>open</code> 开始爬取后触发</li>
<li><code>request_seen</code>检查是否已经访问过，return Flase表没有，True就是访问过了</li>
<li><code>close</code> 停止爬取后触发</li>
</ul>
<p>我们模仿着dupefilter模块，去定制一个专门用于URL去重功能的模块：<br>我们把自己定制的这个判断处理URL模块叫做<code>duplication</code>，然后将dupefilter模块的<em>BaseDupeFilter<em>复制过来<br>把判断URL的类（</em>BaseDupeFilter</em>)我们改名叫做<code>RepeatFilter</code><br>重要：在settings中自己写上<code>DUPEFILTER_CLASS = &quot;项目工程名.duplication.RepeatFilter&quot;</code>（否则不会触发我们自己定制的类）</p>
<p>这样设置后，使用Request对传入URL自动就会执行我们这个类里面的东西了！</p>
<p>自定制定制URL去重功能：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RepeatFilter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>  <span class="comment"># 执行顺序 2</span></span><br><span class="line">        self.visited_set = set()</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span>  <span class="comment"># 执行顺序 1</span></span><br><span class="line">        <span class="keyword">return</span> cls()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span>  <span class="comment">#  执行顺序 4</span></span><br><span class="line">        <span class="keyword">if</span> request.url <span class="keyword">in</span> self.visited_set:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        self.visited_set.add(request.url)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(self)</span>:</span>  <span class="comment"># can return deferred # 执行顺序 3</span></span><br><span class="line">        <span class="comment"># print('open')</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, reason)</span>:</span>  <span class="comment"># can return a deferred # 执行顺序 5</span></span><br><span class="line">        <span class="comment"># print('close')</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, request, spider)</span>:</span>  <span class="comment"># log that a request has been filtered</span></span><br><span class="line">        <span class="comment"># print('log....')</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h2 id><a href="#" class="headerlink" title></a></h2><hr>
<h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>以上是个人学习之路，如有误，欢迎指正！<br><a href="https://www.cnblogs.com/wupeiqi/articles/6229292.html" target="_blank" rel="noopener">参考文献</a></p>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python%E7%88%AC%E8%99%AB/">python爬虫    </a><a class="post-meta__tags" href="/tags/scrapy/">scrapy    </a></div><div class="post_share"><div class="social-share" data-image="/img/scrapy入门_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/10/25/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5/"><img class="next_cover lozad" data-src="/img/topimg.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>python爬虫之如何实现并发</span></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="lv-container" data-id="city" data-uid="MTAyMC80NzI1MC8yMzc1MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div></div><footer style="background-image: url(/img/scrapy入门topimg.jpg)"><div id="footer"><div class="copyright">&copy;2019 By Rhubarb C</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="nightshift fa fa-moon-o" id="nightshift" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script color="0,0,255" opacity="0.7" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/js/canvas-nest.js"></script><script src="/js/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();
</script></body></html>