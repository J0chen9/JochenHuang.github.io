<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Scrapy框架入门 | RhubarbC的博客</title><meta name="description" content="介绍Scrapy框架一些的基础知识和学习心得"><meta name="keywords" content="python爬虫,scrapy,入门"><meta name="author" content="Rhubarb C"><meta name="copyright" content="Rhubarb C"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yoursite.com/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Scrapy框架入门"><meta name="twitter:description" content="介绍Scrapy框架一些的基础知识和学习心得"><meta name="twitter:image" content="http://yoursite.com/img/scrapy入门_cover.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy框架入门"><meta property="og:url" content="http://yoursite.com/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/"><meta property="og:site_name" content="RhubarbC的博客"><meta property="og:description" content="介绍Scrapy框架一些的基础知识和学习心得"><meta property="og:image" content="http://yoursite.com/img/scrapy入门_cover.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="Scrapy框架之缓存与中间件" href="http://yoursite.com/2019/11/01/Scrapy%E6%A1%86%E6%9E%B6%E4%B9%8B%E7%BC%93%E5%AD%98%E4%B8%8E%E4%B8%AD%E9%97%B4%E4%BB%B6/"><link rel="next" title="python爬虫之如何实现并发" href="http://yoursite.com/2019/10/25/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://deehuang.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: {"languages":{"author":"作者: Rhubarb C","link":"链接: http://yoursite.com/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/","source":"来源: RhubarbC的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  copy_copyright_js: true
  
}</script></head><body><canvas class="fireworks"></canvas><div id="header"> <div id="page-header"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">RhubarbC的博客</a></span><i class="fa fa-bars fa-fw toggle-menu pull-right close" aria-hidden="true"></i><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lozad avatar_img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">21</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">34</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">12</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy介绍"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">scrapy介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy安装"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">scrapy安装</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy基础"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">scrapy基础</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#scrapy基本组件"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">scrapy基本组件</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy项目结构及Spider应用简介"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">scrapy项目结构及Spider应用简介</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#项目结构"><span class="toc_mobile_items-number">4.1.</span> <span class="toc_mobile_items-text">项目结构</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#基本命令"><span class="toc_mobile_items-number">4.2.</span> <span class="toc_mobile_items-text">基本命令</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Scrapy数据解析及处理"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">Scrapy数据解析及处理</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#初步数据解析处理"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">初步数据解析处理</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#选择器"><span class="toc_mobile_items-number">5.1.1.</span> <span class="toc_mobile_items-text">选择器</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#递归深度爬取"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text">递归深度爬取</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据格式化处理"><span class="toc_mobile_items-number">5.3.</span> <span class="toc_mobile_items-text">数据格式化处理</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#数据持久化处理"><span class="toc_mobile_items-number">5.4.</span> <span class="toc_mobile_items-text">数据持久化处理</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Scrapy常用功能补充及拓展"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">Scrapy常用功能补充及拓展</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#URL去重（避免重复访问）"><span class="toc_mobile_items-number">6.1.</span> <span class="toc_mobile_items-text">URL去重（避免重复访问）</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#REPDupefilter类"><span class="toc_mobile_items-number">6.1.1.</span> <span class="toc_mobile_items-text">REPDupefilter类</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#自定制URL去重类"><span class="toc_mobile_items-number">6.1.2.</span> <span class="toc_mobile_items-text">自定制URL去重类</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Pipeline补充"><span class="toc_mobile_items-number">6.2.</span> <span class="toc_mobile_items-text">Pipeline补充</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Cookie问题"><span class="toc_mobile_items-number">6.3.</span> <span class="toc_mobile_items-text">Cookie问题</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Scrapy框架拓展"><span class="toc_mobile_items-number">6.4.</span> <span class="toc_mobile_items-text">Scrapy框架拓展</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#Settings配置文件详解"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">Settings配置文件详解</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#scrapy配置文件"><span class="toc_mobile_items-number">7.1.</span> <span class="toc_mobile_items-text">scrapy配置文件</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#自定义scrapy命令"><span class="toc_mobile_items-number">7.2.</span> <span class="toc_mobile_items-text">自定义scrapy命令</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#结语"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text">结语</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy介绍"><span class="toc-number">1.</span> <span class="toc-text">scrapy介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy安装"><span class="toc-number">2.</span> <span class="toc-text">scrapy安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy基础"><span class="toc-number">3.</span> <span class="toc-text">scrapy基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#scrapy基本组件"><span class="toc-number">3.1.</span> <span class="toc-text">scrapy基本组件</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy项目结构及Spider应用简介"><span class="toc-number">4.</span> <span class="toc-text">scrapy项目结构及Spider应用简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#项目结构"><span class="toc-number">4.1.</span> <span class="toc-text">项目结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基本命令"><span class="toc-number">4.2.</span> <span class="toc-text">基本命令</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy数据解析及处理"><span class="toc-number">5.</span> <span class="toc-text">Scrapy数据解析及处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#初步数据解析处理"><span class="toc-number">5.1.</span> <span class="toc-text">初步数据解析处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#选择器"><span class="toc-number">5.1.1.</span> <span class="toc-text">选择器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#递归深度爬取"><span class="toc-number">5.2.</span> <span class="toc-text">递归深度爬取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据格式化处理"><span class="toc-number">5.3.</span> <span class="toc-text">数据格式化处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据持久化处理"><span class="toc-number">5.4.</span> <span class="toc-text">数据持久化处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy常用功能补充及拓展"><span class="toc-number">6.</span> <span class="toc-text">Scrapy常用功能补充及拓展</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#URL去重（避免重复访问）"><span class="toc-number">6.1.</span> <span class="toc-text">URL去重（避免重复访问）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#REPDupefilter类"><span class="toc-number">6.1.1.</span> <span class="toc-text">REPDupefilter类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定制URL去重类"><span class="toc-number">6.1.2.</span> <span class="toc-text">自定制URL去重类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pipeline补充"><span class="toc-number">6.2.</span> <span class="toc-text">Pipeline补充</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cookie问题"><span class="toc-number">6.3.</span> <span class="toc-text">Cookie问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy框架拓展"><span class="toc-number">6.4.</span> <span class="toc-text">Scrapy框架拓展</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Settings配置文件详解"><span class="toc-number">7.</span> <span class="toc-text">Settings配置文件详解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#scrapy配置文件"><span class="toc-number">7.1.</span> <span class="toc-text">scrapy配置文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自定义scrapy命令"><span class="toc-number">7.2.</span> <span class="toc-text">自定义scrapy命令</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#结语"><span class="toc-number">8.</span> <span class="toc-text">结语</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/scrapy入门topimg.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">Scrapy框架入门</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-10-30<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-11-10</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">python网络爬虫</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/Scrapy/">Scrapy</a></span><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">11.9k</span><span class="post-meta__separator">|</span><span>阅读时长: 43 分钟</span><span class="post-meta__separator">|</span><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="scrapy介绍"><a href="#scrapy介绍" class="headerlink" title="scrapy介绍"></a>scrapy介绍</h1><p>python爬虫我们一般使用:<br>requests模块帮助我们发送请求；<br>使用异步IO模块（gevent、Twisted、Tornado、asyncio）帮助我们异步发送请求，提高并发！BeautifulSoup模块帮助我们解析数据，提取我们想要的东西</p>
<p>如果有一个需求，要把一个网站内“所有”的数据提取出来，即我要拿到这个网站内的所有的页面。我就需要用循环或者递归去找到这个网站下所有的url并且进入这个url再做此过程直到找到网页底部。（给一个网站首页我们是可以找到这个网站上所有的页面的）。我们如果通过上面的知识自己去找的话其实还是挺费劲的，特别是如果网页的<strong>深度</strong>很大的话，想想就折磨…….python作为强大的“面向调包”语言，有没有一个模块能帮我们实现这样的功能呢？答案当然是YES!</p>
<p>我们把上面说的例子所实现的功能叫做深度查找，而我们的scrapy框架就给我们提供了这样便捷的功能。<br>拓展：深度查找其实有分广度优先和深度优先<br>广度优先就是先找到这个页面上每个URL都走一遍然后再到下一个深度的URL走一遍<br>深度优先就是对一个页面的一个URL找直接到底，再去另一个URL找到底直到该页面的所有URL全被找到了底</p>
<p>什么是scrapy框架？一般我们叫框架的那必然是集成了许多功能的大型模块。scrapy框架就是集成了我们所学的所有单点功能的整合，就是上面所说的所有模块的功能。不仅如此，它还提供了许多便捷强大的功能，如：深度查找。</p>
<hr>
<h1 id="scrapy安装"><a href="#scrapy安装" class="headerlink" title="scrapy安装"></a>scrapy安装</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Linux</span><br><span class="line">      pip3 install scrapy</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">Windows</span><br><span class="line">      a. pip3 install wheel</span><br><span class="line">      b. 下载twisted http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</span><br><span class="line">      c. 进入下载目录，执行 pip3 install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl</span><br><span class="line">      d. pip3 install scrapy</span><br><span class="line">      e. 下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/</span><br></pre></td></tr></table></figure>

<h1 id="scrapy基础"><a href="#scrapy基础" class="headerlink" title="scrapy基础"></a>scrapy基础</h1><h2 id="scrapy基本组件"><a href="#scrapy基本组件" class="headerlink" title="scrapy基本组件"></a>scrapy基本组件</h2><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。<br>其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。</p>
<p>有没有跟我一样好奇，scrapy内部是怎么做的？</p>
<p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下</p>
<p><img alt="scrapy架构图" data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/scrapy%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg" class="lozad"></p>
<p>图中描述的是Scrapy主要包括了以下组件：</p>
<ul>
<li><strong>引擎(Scrapy)</strong><br>用来处理整个系统的数据流处理, 触发事务(框架核心)；<br>让爬虫深度爬取页面的时候，怎么让爬虫知道自己下一步该走到哪里呢？这就需要引擎帮我们做控制，它主要控制的就是写循环或递归让爬虫能一直深度的执行；引擎还帮我们做很多事情，例如调度任务，引擎去调度器把URL拿过来，然后让虫子去这个URL执行，接着引擎会去调度下载器把页面内容下载下来，虫子从中解析提取自己需要的信息….这些过程都是需要引擎去参与调度的！</li>
<li><strong>调度器(Scheduler)</strong><br>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
<li><strong>下载器(Downloader)</strong><br>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
<li><strong>爬虫(Spiders)</strong><br>爬虫是<strong>主要干活的</strong>, 用于从特定的网页中<strong>提取</strong>自己需要的信息, 即所谓的实体(Item)。<u>用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</u></li>
<li><strong>项目管道(Pipeline)</strong><br>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li><strong>下载器中间件(Downloader Middlewares)</strong><br>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li><strong>爬虫中间件(Spider Middlewares)</strong><br>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li><strong>调度中间件(Scheduler Middewares)</strong><br>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>
<p>可以简单的理解为：<br>调度器（Scheduler）就是一个队列，用于存储URL；<br>引擎（Scrapy）可以认为是一个while循环，不断的从调度器队列<strong>取数据</strong>（URL）然后调度下载器去下载页面让爬虫（Spiders）进行解析提取数据（实体）操作。<br>爬虫（Spiders）主要就是拿到网页内容做数据的解析和提取，提取出来的信息我们就叫实体（Item）<br>项目管道（ItemPipeline）负责处理爬虫爬取到的实体（虫子拿到的数据就丢给他了），进行一些操作，比如放入数据库，保存本地硬盘(这个就叫持久化操作)、对验证进行验证，还有再过滤实体中的一些信息等等<br>总结一下就是：<br>每个爬虫（Spider）都有一个初始的URL（一开始我们自己就要定义好）</p>
<ol>
<li>引擎（scrapy）第一步是把所有的Spider的初始URL放到调度器（Scheduler）中（可以记为引擎是梦开始的地方）</li>
<li>引擎再去用调度器中取URL然后调我们的下载器去下载页面内容</li>
<li>下载器把下载的结果给到我们的爬虫（Spiders）</li>
<li>爬虫拿到下载的结果后接下来要执行什么操作就由我们自己决定了，如果我们想要让某个URL继续操作就把它继续放到调度器就行了（深度爬取）；如果我们要把数据进行持久化（放到数据库）就丢到Pilpeline去</li>
</ol>
<p>实际的框架的执行流程是这样的：</p>
<ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器</li>
<li>下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>
<li>解析出的是链接（URL）,则把URL交给调度器等待抓取</li>
</ol>
<p>整个环节都是通过<strong>引擎</strong>去运行的，所以说引擎是scrapy框架的核心！</p>
<p>实际对框架的使用中，我们就是要对Spider做操作，要做的就两点：</p>
<ol>
<li>第一是指定URL，第二就是解析返回值的内容</li>
<li>解析响应内容<ul>
<li>给调度器，继续进行爬取操作</li>
<li>给item pipeline用于做格式化（item用来做格式化）持久化(pipeline用来做持久化)</li>
</ul>
</li>
</ol>
<h1 id="scrapy项目结构及Spider应用简介"><a href="#scrapy项目结构及Spider应用简介" class="headerlink" title="scrapy项目结构及Spider应用简介"></a>scrapy项目结构及Spider应用简介</h1><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><p>创建一个scrapy工程项目<br><code>scrapy startproject project_name</code>  </p>
<p>然后我们的<em>project_name</em>工程目录下就有了这些东西<br><img alt="scrapy工程目录" data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/scrapy%E5%B7%A5%E7%A8%8B%E7%9B%AE%E5%BD%95.jpg" class="lozad"></p>
<p>文件说明：</p>
<ul>
<li>scrapy.cfg  项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）</li>
<li>items.py   设置数据存储模板，用于结构化数据，如：Django的Model</li>
<li>pipelines   数据处理行为，如：一般结构化的数据持久化</li>
<li>settings.py 配置文件，如：递归的层数、并发数，延迟下载等</li>
<li>spiders    爬虫目录，如：创建文件，编写爬虫规则</li>
</ul>
<p>点进spiders文件发现里面只有一些初始化文件，没有东西（因为我们还没有创建蜘蛛）</p>
<p>此时我们进入该工程目录下，在命令行执行<code>scrapy genspider chouti chouti.com</code>就创建了一个蜘蛛并且给他初始化了URL，此时spiders文件下就多出来了我们创建的蜘蛛<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/1.jpg" class="lozad"></p>
<p>点开这个文件发现，它创建了一个类。类中：<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/2.jpg" class="lozad"></p>
<ul>
<li><code>name</code>属性是我们创建蜘蛛时写的第一个参数，表示爬虫的名字</li>
<li><code>allowed_domins</code>属性是我们创建蜘蛛写的第二个参数，表示支持的域名，可以理解为爬虫给的边界，如果网页内部有别的域名的链接，它会去这里面比较，然后决定爬不爬！（注意的是初始URL不受这个边界限制，因为程序一执行就发过去了，在递归爬取网页里的其他url的时候才会去比较！）</li>
<li><code>start_urls</code>属性默认是基于域名生成的URL，我们可以自己改动这个URL，因为这就是我们要爬取的起始URL</li>
<li><code>parse</code>方法的作用是<u>访问起始URL并获取结果后的回调函数</u>。传入的参数response,就是下载器下载网页的给爬虫返回值对象，它封装了许多东西：<ul>
<li><code>response.url</code>能拿到我们的请求的url，</li>
<li><code>response.text</code>能拿到我们请求的内容</li>
<li><code>response.body</code>能拿到请响应的所有东西（包括请求头）</li>
<li><code>response.meta</code>这个玩意返回的是一个字典{}，包含了很多东西，比如当前深度depth,我们记住{‘depth’:’深度’}就完事</li>
</ul>
</li>
</ul>
<p>定义好起始URL和回调函数后，我们就写好了我们的爬虫了，接下来就可以去执行爬取网站。调用的命令行是<code>scrapy crawl 爬虫名</code>。如果不像看到它的执行日志的话，可以使用<code>scrapy crawl 爬虫名 --nolog</code></p>
<h2 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h2><p>总结一下使用创建scrapy并简单实用爬虫应用的操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy startproject 工程名</span><br><span class="line">	- 在当前目录中创建中创建一个项目文件（类似于Django）</span><br><span class="line">	</span><br><span class="line">cd 工程名</span><br><span class="line"></span><br><span class="line">scrapy genspider 爬虫名 支持爬取的域名</span><br><span class="line">	- 创建爬虫应用</span><br><span class="line">	</span><br><span class="line">#进入spiders目录，打开爬虫名.py进行编辑</span><br><span class="line"></span><br><span class="line">scrapy crawl 爬虫名 --nolog</span><br><span class="line">	- 运行单独爬虫应用</span><br><span class="line">	</span><br><span class="line">scrapy list</span><br><span class="line">	- 展示爬虫应用列表</span><br></pre></td></tr></table></figure>

<p> <em>注意：一般创建爬虫文件时，以网站域名命名</em> </p>
<p>PS: window中去查看<code>response.text</code>时候可能会因为编码问题而无法显示内容，我们需要去改名一下编码内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> sys,os,io</span><br><span class="line">sys.stdout=io.TextIOWrapper(sys.stdout.buffer,encoding=<span class="string">'gb18030'</span>) <span class="comment"># 标准输入输出</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChoutiSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">	name = <span class="string">'chouti'</span></span><br><span class="line">	allowed_domains = [<span class="string">'chouti.com'</span>]</span><br><span class="line">	start_urls = [<span class="string">'http://dig.chouti.com/'</span>]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">		print(response.url)</span><br><span class="line">        print(response.text)</span><br><span class="line">		content=str(response.body,encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">		print(content)</span><br></pre></td></tr></table></figure>

<p>这就完成了我们爬取功能！</p>
<p>爬虫标准的三步走：下载网页—&gt;解析提取数据—&gt;保存数据(放到数据库去)</p>
<p>接下来我们就来讲一下怎么使用scrapy解析数据</p>
<h1 id="Scrapy数据解析及处理"><a href="#Scrapy数据解析及处理" class="headerlink" title="Scrapy数据解析及处理"></a>Scrapy数据解析及处理</h1><p>在我们<code>parse</code>方法中，我们拿到下载器下载后返回给我们的页面数据，在这里我们可以做一些简单的初步数据处理。然后再决定接下来要做什么处理操作：进一步爬取（实体（提取后的信息）是url）；格式化初步处理的数据然后再进一步去处理这些数据或者是数据持久化（放进数据库）<br>但是所有的操作都需要基于我们初步的数据解析才能往下操作。</p>
<h2 id="初步数据解析处理"><a href="#初步数据解析处理" class="headerlink" title="初步数据解析处理"></a>初步数据解析处理</h2><p>这里要介绍的是在<code>parse</code>中我们如何对数据的初步简单处理</p>
<h3 id="选择器"><a href="#选择器" class="headerlink" title="选择器"></a>选择器</h3><p>Scrapy使用xpath的选择器来对数据来过滤出我们想要的标签对象和内容</p>
<p>在Scrapy中，有<em>Selector<em>或</em>HtmlXpathSelector<em>用于结构化HTML代码（转换为标签对象）并提供选择器功能，利用它可以*</em>初步*<em>的解析过滤数据（为什么是初步，因为在这里只是做了简单的数据处理，前面介绍scrapy主要组件的时候又说道</em>Item</em> <em>Pipeline</em>组件提供了数据持久化和格式化功能，通过Item统一格式化数据再传给pipeline也可以做更进一步的处理）。</p>
<p>啊?什么是选择器?接触过<em>css</em>和<em>jquery</em>就知道是按语法去匹配标签对象的一种方法！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> sys,os,io</span><br><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> selector,HtmlXpathSelector</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChoutiSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">	name = <span class="string">'chouti'</span></span><br><span class="line">	allowed_domains = [<span class="string">'chouti.com'</span>]</span><br><span class="line">	start_urls = [<span class="string">'http://dig.chouti.com/'</span>]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 标签对象列表，Selector就是把文档变成标签对</span></span><br><span class="line">		hxs=Selector(response=response).\\</span><br><span class="line">        xpath(<span class="string">'//div[@id"=1"]/div[class="item"]'</span>)\\  <span class="comment"># 选择器，用来找标签对象</span></span><br><span class="line">        .extract()  <span class="comment"># 拿到的是标签对象，如果想拿字符串内容就要使用extract方法</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> hxs:</span><br><span class="line">            print(i)</span><br><span class="line">        <span class="comment">### 更多选择器的使用例子 ### </span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[2]')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[@id]') #找到所有有id的a标签</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[@id="i1"]')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[@href="link.html"][@id="i1"]')</span></span><br><span class="line"><span class="comment"># print(hxs) # 双条件的运用</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[contains(@href, "link")]')</span></span><br><span class="line"><span class="comment"># print(hxs) # 包含link就匹配查找，跟字符串里面的in操作是一样的</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[starts-with(@href, "link")]')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]')</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]/text()').extract()</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]/@href').extract()</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('/html/body/ul/li/a/@href').extract()</span></span><br><span class="line"><span class="comment"># print(hxs) # 不越级一个孩子一个孩子往下找</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//body/ul/li/a/@href').extract_first()</span></span><br><span class="line"><span class="comment"># print(hxs) # //直接越级在整个文档对象的子子孙孙去找</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># ul_list = Selector(response=response).xpath('//body/ul/li')</span></span><br><span class="line"><span class="comment"># for item in ul_list:</span></span><br><span class="line"><span class="comment">#     v = item.xpath('./a/span') # 在当前基础上往下去找</span></span><br><span class="line"><span class="comment">#     # 或</span></span><br><span class="line"><span class="comment">#     # v = item.xpath('a/span')</span></span><br><span class="line"><span class="comment">#     # 或</span></span><br><span class="line"><span class="comment">#     # v = item.xpath('*/a/span')</span></span><br><span class="line"><span class="comment">#     print(v)</span></span><br></pre></td></tr></table></figure>

<p>HtmlXpathSelector模块和Selector的使用方式是一样的，只不过前者的选择器使用的是<code>select(&#39;这里用xpath&#39;)</code>方法。</p>
<p>xpath选择器的基本使用：<br>Selecotor可以把我们的<strong>HTML文档变成标签对象</strong>，通过它下面的<code>xpath()</code>方法可以去选择我们想要的标签对象<u>列表列表列表</u>。<br>想要把<u>标签对象列表转换成字符串列表</u>，需要使用<code>extract()</code>方法</p>
<p><code>//</code>表示从整个Html对象子子孙孙去找标签，<code>//div</code> 就是去整个<em>html</em>文档去找<em>div</em>标签<br><code>.//</code>当前对象的子孙中去找<br><code>/</code>表示去找子标签  如 ./表示的是从当前标签的孩子里面去找<br><code>[@]</code>筛选条件，表示标签的属性  如<code>//div[@id=1]/div[class=&#39;item&#39;]</code> 表示在所有的标签中找到<em>id=1</em>的标签，然后再找到它下面<em>class</em>属性为<em>item</em>的标签</p>
<p>如果我们想从某个已经获取到的标签对象去找它子子孙孙对象需要用 <code>.//</code>去找，如：假设obj是我们已经通过Selector选择出来的某个div标签对象 ,通过<code>obj.xpath(.//a[@class=&#39;hidden-content&#39;])</code>就可以拿到该对象下面的所有属性<em>class=‘hidden-content’</em>的<strong>标签对象列表</strong></p>
<p>如果我们想要拿到对象列表中的某一个元素，可以通过索引的方法，如<code>//a[2]</code>，表示在找到Html中所有的a标签然后返回第二个a标签元素。</p>
<p>如果我们拿到标签内的文本内容，需要在xpath方法里面/text()。如：<code>obj.xpath(&quot;.//a[@class=&#39;hidden-content&#39;]/text()&quot;)</code>表示拿到该对象下面的所有属性<em>class=‘hidden-content’</em>的标签对象的文本列表。</p>
<p>如果想拿到字符串内容可以使用<code>extract()</code>方法。但是通过选择器筛选的结果都是标签对象列表。所以这个方法返回<strong>字符串内容的也是一个列表</strong>，因为我们是对对象列表进行的<em>extract</em>。如果我们知道该列表只有一个元素，即我们只需要取里面的第一个元素，可以使用<code>extract_first()</code>就可以把对象转换为字符串且只拿列表的第一个元素</p>
<p>小总结一下：</p>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/3.jpg" class="lozad"></p>
<p> 补充：<br>如果要拿标签对象中属性可以使用 <code>/@xxx</code>  如：<code>obj.xpath(&#39;//div[@id=&quot;dig_lcpage&quot;]/a/@href&#39;)</code>拿到<code>id=dig_lcage</code>的<code>div</code>标签对象下的所有a标签的<strong><em>href</em>属性值</strong><br><em>PS：<code>//a[@id]</code>和<code>//a/@id</code>是有区别的，前者是知道有所具有id属性的a标签。后者是找到所有a标签上的id的<u>值</u></em></p>
<p>如果我们有需求查询的是某个属性是的值是以什么什么开头的，xpath提供了这样的方法<code>starts-with(@属性名,&#39;属性值&#39;)</code>如：<code>Selector(response=response).xpath(&#39;//a[starts-with(@href,&quot;/all/hot/recent/&quot;)]&#39;/@href)</code>，这就表示在整个Html标签对象下找到<em>href</em><u>属性内容</u>是以<code>/all/hot/recent/</code>开头的所有<em>a</em>标签对象，并获取它们的<em>href</em>属性值(为了加深上面获取属性值的方法，所以这里例子复杂了点~)</p>
<p>选择器还支持正则表达式去模糊匹配要选择的对象！固定写法<code>re:text(&#39;属性名，&#39;模糊匹配的属性值&#39;)</code>如：<code>Selector(response=response).xpath(&#39;//a[re:text(@id,&quot;i\d+)]&quot;)</code></p>
<hr>
<p>这里大致的就完成了我们对数据的初步处理操作，接下来就是我们按需求对实体（提取出来的信息），是进一步过滤处理还是再进一步的深度爬取的处理操作呢？亦或者是把数据放入数据库中。</p>
<h2 id="递归深度爬取"><a href="#递归深度爬取" class="headerlink" title="递归深度爬取"></a>递归深度爬取</h2><p>首先要始终明确记住一点是：在scarpy组件重爬虫（Spider）的就是<strong>主要干活的</strong>, 用于从特定的网页中<strong>提取</strong>自己需要的信息, 即所谓的实体(Item)。<u>用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</u></p>
<p>如果我们的需求是爬取抽屉网站上里所有的页码，那我们上面提取的实体就是URL。回忆之前说的scrapy工作流程！我们只需要把URL再压入调度器，然后让下载器去下载，最后把下载内容返回给爬虫即可完成进一步的爬取工作。</p>
<p>因为我们的需求是找到所有的页码内容，所以我们每次的爬取的初步处理都是一样的：提取URL实体，所以就可以采用递归爬取(执行一样的处理函数)的方式去实现我们的需求。</p>
<p>怎么把任务丢到调动器呢？<br><code>from scrapy.http improt Request</code><br><code>yield Request(url=url)</code><br>完成！这样scrapy就会把这个Request对象添加到调度器里面了，并且下载器会自动去下载（这个中间过程是引擎做的）<br>Request是一个封装用户请求的类，在回调函数中yield该对象表示继续访问<br><strong>要注意的是：这里必须要<code>yield</code>一下，引擎看到<code>yield</code>就会自动的把Request放到调度器去了。</strong></p>
<p>但是有一个问题，下载的内容它会交给哪个爬虫去处理呢？它自己是不知道的，所以我们需要再指定下回调函数（可以自己自定义要执行的回调函数，但是我们这里需要使用深度递归查找的方法）。<br><code>Request(url=url,callback=self.parse)</code> <em>self.parse</em>就是指我这个爬虫的parse函数。这样下载器下载完后就调度一下self.parse方法。</p>
<p>所以记住！<code>yelid Request(url,callback)</code>表示的就是把任务压到调度器然后下载任务，下载完后调用我们的<em>callback</em>函数</p>
<p>这里容易有一个疑问，那么我们的初始start_url是怎么知道我们要去执行我们的<code>parse</code>函数的。于是我看了一下源码，它是这样实现的<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/4.jpg" class="lozad"><br>程序实行它会遍历我们的<em>start_urls</em>列表拿到起始url，然后把每个utl传入执行<code>make_requests_from_ur()</code>方法。再看看这个方法在做了什么<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/5.jpg" class="lozad"><br>发现，它也是创建了<em>Request</em>对象把url传进去（放入调度器并下载），这里的<code>callback</code>默认情况就是去执行我们的<code>self.parse</code>。</p>
<p>它其实就是做了这样一件事！<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/6.jpg" class="lozad"><br>通过它我们也可以重写parse方法，不一定要用parse。即自己指定最开始请求的方法<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/1572463413(1).jpg" class="lozad"></p>
<p>所以通过：<br><code>from scrapy.http improt Request</code><br><code>yeild Request(url=url,callback=self.parse)</code><br>我们就实现了我们的递归实现深度爬取了！</p>
<p>理解下深度爬取：因为页码一般都是动态显示的（我们首页可能就只能看到1-10的页面，当点到7后，页面11才会出现,10的时候出现14页）<em>PS:我们的爬虫会在指定的深度上自动再走一层！</em><br>当前爬取的页面我们就叫深度0。基于这个页面去点进去的所有页码就是深度2。<br>深度0能拿到的也就10个页码，但是会再走一层即拿到深度1的东西，所以爬到14个页码<br>深度1是从0点击去的页码，它最多点到10,所以只能拿到14个页码，但是它会再走一层，即拿到深度2的东西，所以爬到18个页码<br>深度2就是基于深度1拿到14个页码，所以能拿到18个页面，再走一层就是22页码 以此类推…….</p>
<p>PS:我们可以在 <em>settings.py</em>中设置<em>DEPTH_LIMIT = x</em>来指定“递归”的层数。<br>DEPTH_LIMIT默认是0表示的是无限深度。</p>
<p>总结：<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/7.jpg" class="lozad"></p>
<p>深度查找就是反复的不停的执行“一个”操作，直到找完了才停止！</p>
<h2 id="数据格式化处理"><a href="#数据格式化处理" class="headerlink" title="数据格式化处理"></a>数据格式化处理</h2><p><strong>scarpy的数据格式化是通过items完成的。</strong></p>
<p>在工程目录的items就是一个类，它默认是以我们的工程名命名，然后啥都没有做，如：<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/8.jpg" class="lozad"></p>
<p>我们说到上述<code>parse</code>中的实例只是简单的处理，所以在<code>parse</code>方法中直接处理。如果对于想要获取更多的数据处理，则可以利用Scrapy的items将数据格式化，然后统一交由pipelines来处理 。</p>
<p>也就是说在parse中处理完的数据就丢到item，它会帮我们自动的将数据格式化，做统一的管理</p>
<p>怎么格式化？就是把我们的传入的数据和它的变量做成一个字典，如：<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/11.jpg" class="lozad"></p>
<p>items就帮我们把处理的数据变成{title:obj,href:obj}这样的字典数据。</p>
<p>这样记住就行：我们在Parse筛选后的数据可以直接丢到item对象去，item就是来做一个临时的统一化的数据管理的！</p>
<h2 id="数据持久化处理"><a href="#数据持久化处理" class="headerlink" title="数据持久化处理"></a>数据持久化处理</h2><p><strong>scrapy的数据持久化是基于pipeline去做的</strong></p>
<p>Pipeline会从items拿到格式化的所有数据，然后进行数据持久化</p>
<p>使用<code>yield item_obj</code>scrapy就会自动的把item对象传递给pipeline</p>
<p>在工程目录的items就是一个类，它默认是以我们的工程名命名,它下面有一个<code>process_item()</code>方法：</p>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/9.jpg" class="lozad"></p>
<p>scrapy会把item对象和spider对象传给它</p>
<p>注意：<strong>要使用pipeline需要在settings里面的</strong><code>ITEM_PIPELINES</code>注册。可以注册多个pipelines,就是说在pipelines模块里面我们可以写多个pipeline类，只要在这里注册上就行了，但是它们是有执行顺序的。每行后面的整型值，确定了他们运行的顺序，item按数字从<strong>低到高</strong>的顺序（小的优先），通过pipeline，通常将这些数字定义在0-1000范围内。<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/10.jpg" class="lozad"></p>
<p>对数据做持久化处理（爬取图片的案例）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'xiaohua.txt'</span>, <span class="string">'w'</span>) </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        v = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        self.file.write(v)</span><br><span class="line">        self.file.write(<span class="string">'\n'</span>)</span><br><span class="line">        self.file.flush()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'imgs'</span>):</span><br><span class="line">            os.makedirs(<span class="string">'imgs'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        response = requests.get(item[<span class="string">'url'</span>], stream=<span class="literal">True</span>)</span><br><span class="line">        file_name = <span class="string">'%s_%s.jpg'</span> % (item[<span class="string">'name'</span>], item[<span class="string">'school'</span>])</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(<span class="string">'imgs'</span>, file_name), mode=<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>要明确一点的是：只要注册了Pipelines，那么所有的爬虫都会去执行这些pipelines。如果我们想针对每个爬虫去做不同的操作，我们可以对pipelines传入的spider做判断例如：<code>if spider.name == &#39;baidu&#39;</code>就行了，这也是它为什么会被传进来。</p>
<p>上面我们有说到parse只是做简单的处理，在pipeline我们也能对处理的数据再进一步的做处理。<strong>所以前面parse的处理我才叫初步处理</strong>。但是是否再做处理看个需求，所以<strong>初步</strong>这个词也是相对的，不要混乱…..</p>
<p>总结一下我们使用scarpy大致的流程:</p>
<ol>
<li>通过选择器采集我们想要的数据（要做数据再处理或持久化工作） —&gt;  把每个实体(提取出来的数据)分装到我们的<em>item</em>对象里面  —&gt; 传给Piperline(<code>yield item</code>对象)，让<em>Piperline</em>帮我们做处理 </li>
<li>通过选择器采集我们想要的数据（要做深度爬取工作）—&gt; 把实体（url）封装到<em>Request</em>对象中，并设置回调函数 <code>Request(url=url,callback=self.parse/other_fun)</code>—&gt;然后放到调度器去 (yield Request对象)，引擎自动帮我们调度并下载返回给爬虫执行回调</li>
</ol>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/12.jpg" class="lozad"></p>
<h1 id="Scrapy常用功能补充及拓展"><a href="#Scrapy常用功能补充及拓展" class="headerlink" title="Scrapy常用功能补充及拓展"></a>Scrapy常用功能补充及拓展</h1><h2 id="URL去重（避免重复访问）"><a href="#URL去重（避免重复访问）" class="headerlink" title="URL去重（避免重复访问）"></a>URL去重（避免重复访问）</h2><p>在进行深度爬取的时候，对爬取回来的URL去重是一个常见且重要的操作。Scrapy里面给我们提供方便的URL去重操作的功能。只要设置Request中的<code>Request（dont_filter=flase）</code>,就会帮我们自动去重（它默认就是false！即默认就会帮我们去重）</p>
<h3 id="REPDupefilter类"><a href="#REPDupefilter类" class="headerlink" title="REPDupefilter类"></a>REPDupefilter类</h3><p>单单设置一下dont_filter参数就能帮我们做去重的事情，这也太方便了！但是我十分好奇它是怎么实现的：<br>scrapy中的查看<code>Request</code>对象的参数，发现它默认会调用一个类去帮我们对传入URL做去重的事情：<br><code>from scrapy.dupefilter import RFPDupeFilter</code></p>
<p>看一下这类里面有什么<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/13.jpg" class="lozad"></p>
<p>图中的<code>request_seen</code>功能就是这个请求是否已经被查看过了，return False表示的是没有查看过，所以会丢到调度器去爬取，return True表示已经查看过了，在这里就会被过滤掉不会丢到调度器去（BaseDupeFilter这是一个基类，看样子是做归一化设计的）<br>而默认情况下对URL的判断去重处理是实际上通过这个类去做的<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/14.jpg" class="lozad"></p>
<p>在执行scrapy程序的时候会自动把它读出来，但是它默认的实现的URL去重功能是把URL放到文件里面去然后再对比文件里的URL是否已经存在，如果我们不想放在文件里去而是直接在缓存里对比或者是放到数据库去的话怎么办呢？</p>
<h3 id="自定制URL去重类"><a href="#自定制URL去重类" class="headerlink" title="自定制URL去重类"></a>自定制URL去重类</h3><p>上面的意思就是，默认给我提供的去重功能定义的比较死板！如果我们想灵活处理URL的存储方式，我们可以自己去定制这个类！去实现自定制的去重功能</p>
<p>scrapy默认使用 scrapy.dupefilter.RFPDupeFilter 进行去重，相关配置有(默认在在settings里是这样配置的，我们也可以对它进行改写)：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy.dupefilter.RFPDupeFilter'</span></span><br><span class="line">DUPEFILTER_DEBUG = <span class="literal">False</span></span><br><span class="line">JOBDIR = <span class="string">"保存范文记录的日志路径，如：/root/"</span> <span class="comment"># 最终路径为 /root/requests.seen</span></span><br></pre></td></tr></table></figure>

<p>通过观察源码，发现RFPDupeFilter类中方法的作用是这样的（执行顺序也如下面所示）：</p>
<ul>
<li>通过<code>from_settings</code>来创建对象</li>
<li><code>__init__</code>进行对象初始化</li>
<li><code>open</code> 开始爬取请求时触发</li>
<li><code>request_seen</code>检查是否已经访问过，return Flase表没有，True就是访问过了</li>
<li><code>close</code> 停止爬取后触发</li>
</ul>
<p>我们模仿着dupefilter模块，去定制一个专门用于URL去重功能的模块：<br>我们把自己定制的这个判断处理URL模块叫做<code>duplication</code>，然后将dupefilter模块的<em>BaseDupeFilter<em>复制过来<br>把判断URL的类（</em>BaseDupeFilter</em>)我们改名叫做<code>RepeatFilter</code><br>重要：在settings中自己写上<code>DUPEFILTER_CLASS = &quot;项目工程名.duplication.RepeatFilter&quot;</code>（否则不会触发我们自己定制的类）</p>
<p>这样设置后，使用Request对传入URL自动就会执行我们这个类里面的东西了！</p>
<p>自定制定制URL去重功能：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RepeatUrl</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        执行顺序 2</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.visited_url = set()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化时，调用</span></span><br><span class="line"><span class="string">        执行顺序 1</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> cls()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        检测当前请求是否已经被访问过</span></span><br><span class="line"><span class="string">        :return: True表示已经访问过；False表示未访问过</span></span><br><span class="line"><span class="string">        执行顺序 4</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> request.url <span class="keyword">in</span> self.visited_url:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        self.visited_url.add(request.url)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        开始爬去请求时，调用</span></span><br><span class="line"><span class="string">        执行顺序 3</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'open replication'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, reason)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        结束爬虫爬取时，调用</span></span><br><span class="line"><span class="string">        执行顺序 5</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'close replication'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        记录日志</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'repeat'</span>, request.url)</span><br></pre></td></tr></table></figure>

<h2 id="Pipeline补充"><a href="#Pipeline补充" class="headerlink" title="Pipeline补充"></a>Pipeline补充</h2><p>上面只介绍了通过yield item对象，就会去触发pipelines中的<code>process_item(item,spider)</code>方法，在里面可以做数据的<u>持久化处理</u>。<br>但其实我们的pipelines不只有process_item方法，还有两个方法是可以被<strong>按顺序</strong>的触发：<code>open_spider(spider)</code>和<code>close_spider(spider)</code><br>这两个方法会分别在爬虫开始执行时和爬虫关闭的时候被调用。<br>啊?什么时候爬虫开始执行什么时候关闭？执行parse回调函数就是爬虫开始执行了！（Spaider组件就是处理数据的）,等函数执行完了就叫爬虫关闭了呀！</p>
<p>来看案例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,v)</span>:</span></span><br><span class="line">        self.value = v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 操作并进行持久化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># return表示会被后续的pipeline继续处理</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># raise DropItem()</span></span><br><span class="line">		<span class="comment"># 表示将item丢弃，不会被后续pipeline处理</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        爬虫开始执行时，调用 </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'000000'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        爬虫关闭时，被调用</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'111111'</span>)</span><br></pre></td></tr></table></figure>

<p>如果我们希望把数据持久化到数据库。我们需要用户名和密码打开数据库，这些东西我们一般都是写在settings里面。除了使用到数据库我们可能还有很多的需求，需要去找settings的配置信息。Scrapy的pipelines也考虑到了这点<br>我们可以基于<code>from_crawler(cls,crawler)</code>方法做扩展功能，在初始化pipeline类的时候，它会调用这个方法来初始化对象。而这个方法会传进来的一个<code>crawler</code>参数，在里面就有配置文件的信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,v)</span>:</span></span><br><span class="line">        self.value = v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 操作并进行持久化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># return表示会被后续的pipeline继续处理</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># raise DropItem()</span></span><br><span class="line">		<span class="comment"># 表示将item丢弃，不会被后续pipeline处理</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化时候，用于创建pipeline对象</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        val = crawler.settings.get(<span class="string">'MMMM'</span>)<span class="comment"># 配置文件中的变量名称且必须大写</span></span><br><span class="line">        <span class="keyword">return</span> cls(val)</span><br></pre></td></tr></table></figure>

<p>通过<code>val = crawler.settings.get(&#39;MMM&#39;)</code>就可以拿到变量MMM的内容了(注意get里面是配置文件的变量名称且<strong>必须大写</strong>)，然后把获取的值在创建对象的时候传进去<code>return cls(val)</code>。初始化对象就会触发<code>__init__</code>方法，我们自己定制init方法去接受这个参数，那么就可以在对象中全局作用了。这里就介完了Pipeline四个方法，它们都各司其职。</p>
<p>还有一点要补充的是:如果有多个<em>pipeline</em>类，它会按照我们设置的权重按顺序执行。但是需要在上一个<em>pipeline</em>执行<em>process_item</em>后最后返回<em>return item</em>，下一个<em>pipeline</em>才会继续执行<br>如果不<em>returen</em>（这样显得不专业很LOW不建议直接不写）或是使用<code>rasise DropItem()</code>（需要导入<code>from scrapy.exceptions import DropItem</code>）的话，就表示把item丢弃，后面的<em>pipeline</em>不会继续进行处理了！</p>
<p>通过上面两个拓展方法有意思的发现：<br>scrapy的组件类中都是使用这种利用静态方法<code>@classmethod</code>创建对象的方式！而不是我们传统的在外部直接通过<code>类名()</code>来创建对象！<br>使用这样的方式创建对象的好处不知大家有没细品出来。就是为了方便我们去在创建对象前做拓展，我们可以自己传各种各样的参数进去！！然后自定制<code>__init__</code>方法去接收。整成我们需要在对象中全局使用的数据或者功能。</p>
<h2 id="Cookie问题"><a href="#Cookie问题" class="headerlink" title="Cookie问题"></a>Cookie问题</h2><p>我们写爬虫经常需要涉及登陆问题，就需要携带Cookie，请求体有时候还需要请求头（Content-Type用于后台数据解析）才能成功</p>
<p>首先我们先补充一个知识点，response响应对象中有request属性即<code>response.request</code>它封装的是我们这个<u>响应所发的请求</u></p>
<p>要想获取请求返回cookie</p>
<ul>
<li>首先我们需要导入一个类<code>from scrapy.http cookies import CookieJar</code></li>
<li>创建cookie对象<code>cookie_obj = CookieJar()</code></li>
<li>调用 对象<em>extract_cookies<em>的方法把</em>response<em>（响应的所有内容对象）和</em>response.request</em>(响应对象的请求)传进去<code>cookie_obj.extract_cookies(response,response.request)</code></li>
<li><code>cookie_obj.__cookies</code>就拿到了cookie的所有内容</li>
</ul>
<p>拿到cookie，我们尝试通过爬虫登陆页面。通过Request对象，设置里面这些参数<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/15.jpg" class="lozad"></p>
<p>body就是我们要传的数据(相当于requests模块中的data参数)。注意：它不可以穿字典格式，只能是<code>k1=v1&amp;k2=v2</code>这样的格式（其实请求体中的数据最终都会被转换成key1=val1&amp;key2=val2发过去的！我们requests模块能传字典是内部帮我们处理了字典而已）</p>
<p>自动登陆抽屉并点赞</p>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/16.jpg" class="lozad"></p>
<h2 id="Scrapy框架拓展"><a href="#Scrapy框架拓展" class="headerlink" title="Scrapy框架拓展"></a>Scrapy框架拓展</h2><p>对于Scrapy执行流程的每一个环节，Scrapy都给我提供了自定制操作的功能</p>
<p>通过Settings里面的<code>EXTENSIONS={}</code>去设置要拓展Scrapy使用的模块<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/1572540997.jpg" class="lozad"><br>它跟注册pipeline的写法有点相似（{执行类：权重}）。程序一执行它会去执行这里面的内容</p>
<p>scrapy默认使用的是TelentConsle类进行指定位置注册定制操作，我们来看一下它在里面做了什么<br><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/1572541138(1).jpg" class="lozad"></p>
<p>这里面真正有用的其实就是<em>from_crawler</em><br>和<em>init</em>方法,下面的那些方法都是通过它们来”注册“的<br>scrapy拓展就是通过在<code>from_crawler(cls,crawler)和__init()</code>中给钩子（信号）挂东西（操作）来全局的影响爬虫的行为<br>上面大白话专业点说应该<strong>是在指定信号上注册操作</strong>（当信号被触发，会把信号上的所有操作都执行一遍）</p>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/1572541501(1).jpg" class="lozad"></p>
<p><em>PS：”钩子“的专业术语叫做*</em>信号<strong>*<br>**信号</strong>有这些：</p>
<p><img alt="17" data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/D:%5CDeeHuang%5Cblog%5Csource_posts%5CScrapy%E6%A1%86%E6%9E%B6%5C17.png" class="lozad"></p>
<p>通过字面意思就很好理解：<br>engine_started 表示引擎开始的时候<br>engine_started 引擎结束的时候<br>spider_idle 爬虫空闲的时候<br>spider_closed 爬虫关闭的时候<br>spider_error 爬虫错误的时候<br>request_scheduled 请求压入调度器的时候<br>response_received 接收到响应的时候<br>response_downloaded 下载响应内容的时候（先下载再给Spaider接收）<br>item_scraped 数据格式化的时候，即yeild item对象的时候<br>item_dropped 在Pipeline持久化数据抛出rasie dropItem异常的时候</p>
<p>基于上面所知道的东西，我们知道自定义扩展时，利用信号在指定位置注册制定操作 。就可以来自定制scrapy拓展模块</p>
<p><img alt data-src="/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/18.jpg" class="lozad"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可复制代码实例</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyExtension</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.value = value</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        val = crawler.settings.getint(<span class="string">'MMMM'</span>)</span><br><span class="line">        ext = cls(val)</span><br><span class="line"></span><br><span class="line">        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ext</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'open'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'close'</span>)</span><br></pre></td></tr></table></figure>

<p>在settings注册模块，设置权重（<strong>权重小的优先</strong>），搞定!</p>
<h1 id="Settings配置文件详解"><a href="#Settings配置文件详解" class="headerlink" title="Settings配置文件详解"></a>Settings配置文件详解</h1><h2 id="scrapy配置文件"><a href="#scrapy配置文件" class="headerlink" title="scrapy配置文件"></a>scrapy配置文件</h2><p>上面我们大概学习掌握scrapy框架的基本使用和一些拓展方法。我们学习的时候很多东西其实都是基于我们的settings完成的，接下来来聊聊settings配置文件里面都有些什么东西</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Scrapy settings for step8_king project</span><br><span class="line">#</span><br><span class="line"># For simplicity, this file contains only settings considered important or</span><br><span class="line"># commonly used. You can find more settings consulting the documentation:</span><br><span class="line">#</span><br><span class="line">#     http://doc.scrapy.org/en/latest/topics/settings.html</span><br><span class="line">#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span><br><span class="line">#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span><br><span class="line"></span><br><span class="line"># 1. 爬虫名称</span><br><span class="line">BOT_NAME = &apos;step8_king&apos;</span><br><span class="line"></span><br><span class="line"># 2. 爬虫应用路径</span><br><span class="line">SPIDER_MODULES = [&apos;step8_king.spiders&apos;]</span><br><span class="line">NEWSPIDER_MODULE = &apos;step8_king.spiders&apos;</span><br><span class="line"></span><br><span class="line"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span><br><span class="line"># 3. 客户端 user-agent请求头（爬虫名会放在这里面提交出去），我们可以自己指定它，来破反爬</span><br><span class="line"># USER_AGENT = &apos;step8_king (+http://www.yourdomain.com)&apos; </span><br><span class="line"></span><br><span class="line"># Obey robots.txt rules</span><br><span class="line"># 4. 禁止爬虫配置（每个网站上都有自己的爬虫规则写在robots上，上面规定了允许谁爬不许谁爬）</span><br><span class="line"># ROBOTSTXT_OBEY = False </span><br><span class="line"># false表示的就是无视robots规则</span><br><span class="line"></span><br><span class="line"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="line"># 5. 并发请求数（有些网站会对爬虫并发数做限制）</span><br><span class="line"># CONCURRENT_REQUESTS = 4</span><br><span class="line"></span><br><span class="line"># Configure a delay for requests for the same website (default: 0)</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</span><br><span class="line"># See also autothrottle settings and docs</span><br><span class="line"># 6. 延迟下载秒数（有些网站会对爬虫访问速度做限制，访问的太快就封你号）</span><br><span class="line"># DOWNLOAD_DELAY = 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># The download delay setting will honor only one of:</span><br><span class="line"># 7. 单域名访问并发数，并且延迟下次秒数也应用在每个域名（对于一个域名来说可以有很多IP所以并发数会相乘）</span><br><span class="line"># CONCURRENT_REQUESTS_PER_DOMAIN = 2</span><br><span class="line"># 单IP访问并发数，如果有值则忽略：CONCURRENT_REQUESTS_PER_DOMAIN，并且延迟下次秒数也应用在每个IP</span><br><span class="line"># CONCURRENT_REQUESTS_PER_IP = 3</span><br><span class="line"></span><br><span class="line"># Disable cookies (enabled by default)</span><br><span class="line"># 8. 是否支持cookie（返回的response是否携带cookie），cookiejar进行操作cookie</span><br><span class="line"># COOKIES_ENABLED = True</span><br><span class="line"># 是否在调试模式中显示设置cookie（调试模式是在执行爬虫不nolog的时候弹出来的一大堆日志）</span><br><span class="line"># COOKIES_DEBUG = True </span><br><span class="line"></span><br><span class="line"># Disable Telnet Console (enabled by default)</span><br><span class="line"># 9. Telnet用于查看当前爬虫的信息，操作爬虫等...</span><br><span class="line">#    使用telnet ip port ，然后通过命令操作</span><br><span class="line"># TELNETCONSOLE_ENABLED = True</span><br><span class="line"># TELNETCONSOLE_HOST = &apos;127.0.0.1&apos;</span><br><span class="line"># TELNETCONSOLE_PORT = [6023,] #6023默认是scrapy使用的端口</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 10. 默认请求头（yield Request里面也可以穿headers）</span><br><span class="line"># Override the default request headers:</span><br><span class="line"># DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">#     &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,</span><br><span class="line">#     &apos;Accept-Language&apos;: &apos;en&apos;,</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Configure item pipelines</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span><br><span class="line"># 11. 定义pipeline处理请求</span><br><span class="line"># ITEM_PIPELINES = &#123;</span><br><span class="line">#    &apos;step8_king.pipelines.JsonPipeline&apos;: 700,</span><br><span class="line">#    &apos;step8_king.pipelines.FilePipeline&apos;: 500,</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 12. 自定义扩展，基于信号进行调用</span><br><span class="line"># Enable or disable extensions</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span><br><span class="line"># EXTENSIONS = &#123;</span><br><span class="line">#     # &apos;step8_king.extensions.MyExtension&apos;: 500,</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 13. 爬虫允许的最大深度，可以通过response.meta查看当前深度；0表示无深度</span><br><span class="line"># DEPTH_LIMIT = 3</span><br><span class="line"></span><br><span class="line"># 14. 爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo</span><br><span class="line">#  深度优先一次找到底再找 广度优先是一层一层找</span><br><span class="line"># 后进先出，深度优先</span><br><span class="line"># DEPTH_PRIORITY = 0</span><br><span class="line"># SCHEDULER_DISK_QUEUE = &apos;scrapy.squeue.PickleLifoDiskQueue&apos;</span><br><span class="line"># SCHEDULER_MEMORY_QUEUE = &apos;scrapy.squeue.LifoMemoryQueue&apos;</span><br><span class="line"></span><br><span class="line"># 先进先出，广度优先（拿到的数据放到队列中，根据深度给爬取url优先级，最外层是0，下一层是-1，再下一层-2...，那么0的优先级肯定是最高的优先入队，所以我们处理的时候优先取到第0层再1，2...）</span><br><span class="line"># DEPTH_PRIORITY = 1</span><br><span class="line"># SCHEDULER_DISK_QUEUE = &apos;scrapy.squeue.PickleFifoDiskQueue&apos;</span><br><span class="line"># SCHEDULER_MEMORY_QUEUE = &apos;scrapy.squeue.FifoMemoryQueue&apos;</span><br><span class="line"></span><br><span class="line"># 15. 调度器队列</span><br><span class="line"># SCHEDULER = &apos;scrapy.core.scheduler.Scheduler&apos;</span><br><span class="line"># from scrapy.core.scheduler import Scheduler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 16. 访问URL去重</span><br><span class="line"># DUPEFILTER_CLASS = &apos;step8_king.duplication.RepeatUrl&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="line"># See http://doc.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">17. 自动限速算法</span><br><span class="line">    from scrapy.contrib.throttle import AutoThrottle</span><br><span class="line">    自动限速设置</span><br><span class="line">    1. 获取最小延迟 DOWNLOAD_DELAY</span><br><span class="line">    2. 获取最大延迟 AUTOTHROTTLE_MAX_DELAY</span><br><span class="line">    3. 设置初始下载延迟 AUTOTHROTTLE_START_DELAY</span><br><span class="line">    4. 当请求下载完成后，获取其&quot;连接&quot;时间 latency，即：请求连接到接受到响应头之间的时间</span><br><span class="line">    5. 用于计算的... AUTOTHROTTLE_TARGET_CONCURRENCY</span><br><span class="line">    target_delay = latency / self.target_concurrency</span><br><span class="line">    new_delay = (slot.delay + target_delay) / 2.0 # 表solt.delay示上一次的延迟时间</span><br><span class="line">    new_delay = max(target_delay, new_delay)</span><br><span class="line">    new_delay = min(max(self.mindelay, new_delay), self.maxdelay)</span><br><span class="line">    slot.delay = new_delay</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># 上面的DOWNLOAD_DELAY设置的时间是每个请求都延迟固定这个时间，很有规律容易猜到是爬虫</span><br><span class="line"># 在这里就是做一个智能的随机延迟时间的爬取</span><br><span class="line"># 开始自动限速</span><br><span class="line"># AUTOTHROTTLE_ENABLED = True</span><br><span class="line"># The initial download delay</span><br><span class="line"># 初始下载延迟</span><br><span class="line"># AUTOTHROTTLE_START_DELAY = 5</span><br><span class="line"># The maximum download delay to be set in case of high latencies</span><br><span class="line"># 最大下载延迟（最多延迟10秒）</span><br><span class="line"># AUTOTHROTTLE_MAX_DELAY = 10</span><br><span class="line"># The average number of requests Scrapy should be sending in parallel to each remote server</span><br><span class="line"># 平均每秒并发数</span><br><span class="line"># AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span><br><span class="line"></span><br><span class="line"># Enable showing throttling stats for every response received:</span><br><span class="line"># 是否显示</span><br><span class="line"># AUTOTHROTTLE_DEBUG = True</span><br><span class="line"></span><br><span class="line"># Enable and configure HTTP caching (disabled by default)</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">18. 启用缓存</span><br><span class="line">    目的用于将已经发送的请求或相应缓存下来，以便以后使用</span><br><span class="line">    </span><br><span class="line">    from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware</span><br><span class="line">    from scrapy.extensions.httpcache import DummyPolicy</span><br><span class="line">    from scrapy.extensions.httpcache import FilesystemCacheStorage</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># 是否启用缓存策略</span><br><span class="line"># 如果我们要发的请求是一样的，可以把爬取的内容放到缓存，那么就不用再去发请求了</span><br><span class="line"># HTTPCACHE_ENABLED = True</span><br><span class="line"></span><br><span class="line"># 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可</span><br><span class="line"># HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.DummyPolicy&quot;</span><br><span class="line"># 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略</span><br><span class="line"># HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.RFC2616Policy&quot;</span><br><span class="line"></span><br><span class="line"># 缓存超时时间</span><br><span class="line"># HTTPCACHE_EXPIRATION_SECS = 0</span><br><span class="line"></span><br><span class="line"># 缓存保存路径</span><br><span class="line"># HTTPCACHE_DIR = &apos;httpcache&apos;</span><br><span class="line"></span><br><span class="line"># 缓存忽略的Http状态码</span><br><span class="line"># HTTPCACHE_IGNORE_HTTP_CODES = []</span><br><span class="line"></span><br><span class="line"># 缓存存储的插件</span><br><span class="line"># HTTPCACHE_STORAGE = &apos;scrapy.extensions.httpcache.FilesystemCacheStorage&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">19. 代理，需要在环境变量中设置</span><br><span class="line">    from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware</span><br><span class="line">    </span><br><span class="line">    方式一：使用默认</span><br><span class="line">        os.environ</span><br><span class="line">        &#123;</span><br><span class="line">            http_proxy:http://root:woshiniba@192.168.11.11:9999/</span><br><span class="line">            https_proxy:http://192.168.11.11:9999/</span><br><span class="line">        &#125;</span><br><span class="line">    方式二：使用自定义下载中间件</span><br><span class="line">    </span><br><span class="line">    def to_bytes(text, encoding=None, errors=&apos;strict&apos;):</span><br><span class="line">        if isinstance(text, bytes):</span><br><span class="line">            return text</span><br><span class="line">        if not isinstance(text, six.string_types):</span><br><span class="line">            raise TypeError(&apos;to_bytes must receive a unicode, str or bytes &apos;</span><br><span class="line">                            &apos;object, got %s&apos; % type(text).__name__)</span><br><span class="line">        if encoding is None:</span><br><span class="line">            encoding = &apos;utf-8&apos;</span><br><span class="line">        return text.encode(encoding, errors)</span><br><span class="line">        </span><br><span class="line">    class ProxyMiddleware(object):</span><br><span class="line">        def process_request(self, request, spider):</span><br><span class="line">            PROXIES = [</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;111.11.228.75:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;120.198.243.22:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;111.8.60.9:8123&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;101.71.27.120:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;122.96.59.104:80&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">                &#123;&apos;ip_port&apos;: &apos;122.224.249.122:8088&apos;, &apos;user_pass&apos;: &apos;&apos;&#125;,</span><br><span class="line">            ]</span><br><span class="line">            proxy = random.choice(PROXIES)</span><br><span class="line">            if proxy[&apos;user_pass&apos;] is not None:</span><br><span class="line">                request.meta[&apos;proxy&apos;] = to_bytes（&quot;http://%s&quot; % proxy[&apos;ip_port&apos;]）</span><br><span class="line">                encoded_user_pass = base64.encodestring(to_bytes(proxy[&apos;user_pass&apos;]))</span><br><span class="line">                request.headers[&apos;Proxy-Authorization&apos;] = to_bytes(&apos;Basic &apos; + encoded_user_pass)</span><br><span class="line">                print &quot;**************ProxyMiddleware have pass************&quot; + proxy[&apos;ip_port&apos;]</span><br><span class="line">            else:</span><br><span class="line">                print &quot;**************ProxyMiddleware no pass************&quot; + proxy[&apos;ip_port&apos;]</span><br><span class="line">                request.meta[&apos;proxy&apos;] = to_bytes(&quot;http://%s&quot; % proxy[&apos;ip_port&apos;])</span><br><span class="line">    </span><br><span class="line">    DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">       &apos;step8_king.middlewares.ProxyMiddleware&apos;: 500,</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">20. Https访问</span><br><span class="line">    Https访问时有两种情况：</span><br><span class="line">    1. 要爬取网站使用的可信任证书(默认支持)</span><br><span class="line">        DOWNLOADER_HTTPCLIENTFACTORY = &quot;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&quot;</span><br><span class="line">        DOWNLOADER_CLIENTCONTEXTFACTORY = &quot;scrapy.core.downloader.contextfactory.ScrapyClientContextFactory&quot;</span><br><span class="line">        </span><br><span class="line">    2. 要爬取网站使用的自定义证书</span><br><span class="line">        DOWNLOADER_HTTPCLIENTFACTORY = &quot;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&quot;</span><br><span class="line">        DOWNLOADER_CLIENTCONTEXTFACTORY = &quot;step8_king.https.MySSLFactory&quot;</span><br><span class="line">        </span><br><span class="line">        # https.py</span><br><span class="line">        from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory</span><br><span class="line">        from twisted.internet.ssl import (optionsForClientTLS, CertificateOptions, PrivateCertificate)</span><br><span class="line">        </span><br><span class="line">        class MySSLFactory(ScrapyClientContextFactory):</span><br><span class="line">            def getCertificateOptions(self):</span><br><span class="line">                from OpenSSL import crypto</span><br><span class="line">                v1 = crypto.load_privatekey(crypto.FILETYPE_PEM, open(&apos;/Users/wupeiqi/client.key.unsecure&apos;, mode=&apos;r&apos;).read())</span><br><span class="line">                v2 = crypto.load_certificate(crypto.FILETYPE_PEM, open(&apos;/Users/wupeiqi/client.pem&apos;, mode=&apos;r&apos;).read())</span><br><span class="line">                return CertificateOptions(</span><br><span class="line">                    privateKey=v1,  # pKey对象</span><br><span class="line">                    certificate=v2,  # X509对象</span><br><span class="line">                    verify=False,</span><br><span class="line">                    method=getattr(self, &apos;method&apos;, getattr(self, &apos;_ssl_method&apos;, None))</span><br><span class="line">                )</span><br><span class="line">    其他：</span><br><span class="line">        相关类</span><br><span class="line">            scrapy.core.downloader.handlers.http.HttpDownloadHandler</span><br><span class="line">            scrapy.core.downloader.webclient.ScrapyHTTPClientFactory</span><br><span class="line">            scrapy.core.downloader.contextfactory.ScrapyClientContextFactory</span><br><span class="line">        相关配置</span><br><span class="line">            DOWNLOADER_HTTPCLIENTFACTORY</span><br><span class="line">            DOWNLOADER_CLIENTCONTEXTFACTORY</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">21. 爬虫中间件</span><br><span class="line">    class SpiderMiddleware(object):</span><br><span class="line"></span><br><span class="line">        def process_spider_input(self,response, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            下载完成，执行，然后交给parse处理</span><br><span class="line">            :param response: </span><br><span class="line">            :param spider: </span><br><span class="line">            :return: </span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            pass</span><br><span class="line">    </span><br><span class="line">        def process_spider_output(self,response, result, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            spider处理完成，返回时调用</span><br><span class="line">            :param response:</span><br><span class="line">            :param result:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            return result</span><br><span class="line">    </span><br><span class="line">        def process_spider_exception(self,response, exception, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            异常调用</span><br><span class="line">            :param response:</span><br><span class="line">            :param exception:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            return None</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">        def process_start_requests(self,start_requests, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            爬虫启动时调用</span><br><span class="line">            :param start_requests:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return: 包含 Request 对象的可迭代对象</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            return start_requests</span><br><span class="line">    </span><br><span class="line">    内置爬虫中间件：</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware&apos;: 50,</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware&apos;: 500,</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.referer.RefererMiddleware&apos;: 700,</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware&apos;: 800,</span><br><span class="line">        &apos;scrapy.contrib.spidermiddleware.depth.DepthMiddleware&apos;: 900,</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># from scrapy.contrib.spidermiddleware.referer import RefererMiddleware</span><br><span class="line"># Enable or disable spider middlewares</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">   # &apos;step8_king.middlewares.SpiderMiddleware&apos;: 543,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">22. 下载中间件</span><br><span class="line">    class DownMiddleware1(object):</span><br><span class="line">        def process_request(self, request, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            请求需要被下载时，经过所有下载器中间件的process_request调用</span><br><span class="line">            :param request:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return:</span><br><span class="line">                None,继续后续中间件去下载；</span><br><span class="line">                Response对象，停止process_request的执行，开始执行process_response</span><br><span class="line">                Request对象，停止中间件的执行，将Request重新调度器</span><br><span class="line">                raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            pass</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">        def process_response(self, request, response, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            spider处理完成，返回时调用</span><br><span class="line">            :param response:</span><br><span class="line">            :param result:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return:</span><br><span class="line">                Response 对象：转交给其他中间件process_response</span><br><span class="line">                Request 对象：停止中间件，request会被重新调度下载</span><br><span class="line">                raise IgnoreRequest 异常：调用Request.errback</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            print(&apos;response1&apos;)</span><br><span class="line">            return response</span><br><span class="line">    </span><br><span class="line">        def process_exception(self, request, exception, spider):</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span><br><span class="line">            :param response:</span><br><span class="line">            :param exception:</span><br><span class="line">            :param spider:</span><br><span class="line">            :return:</span><br><span class="line">                None：继续交给后续中间件处理异常；</span><br><span class="line">                Response对象：停止后续process_exception方法</span><br><span class="line">                Request对象：停止中间件，request将会被重新调用下载</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            return None</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    默认下载中间件</span><br><span class="line">    &#123;</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware&apos;: 100,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware&apos;: 300,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware&apos;: 350,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&apos;: 400,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.retry.RetryMiddleware&apos;: 500,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware&apos;: 550,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware&apos;: 580,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware&apos;: 590,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware&apos;: 600,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware&apos;: 700,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware&apos;: 750,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware&apos;: 830,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.stats.DownloaderStats&apos;: 850,</span><br><span class="line">        &apos;scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware&apos;: 900,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware</span><br><span class="line"># Enable or disable downloader middlewares</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span><br><span class="line"># DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">#    &apos;step8_king.middlewares.DownMiddleware1&apos;: 100,</span><br><span class="line">#    &apos;step8_king.middlewares.DownMiddleware2&apos;: 500,</span><br><span class="line"># &#125;</span><br></pre></td></tr></table></figure>

<h2 id="自定义scrapy命令"><a href="#自定义scrapy命令" class="headerlink" title="自定义scrapy命令"></a>自定义scrapy命令</h2><p>在执行爬虫的时候我们通过命令<code>scrapy crawl 爬虫名</code>的方式来运行爬虫，但是我们发现这种方式只能运行一个爬虫。如果我们有需求是通过命令执行我们多个爬虫的话，这个时候就需要我们自己去定义scrapy的命令了。</p>
<ul>
<li><p>在spiders同级创建任意目录（即工程目录下），如：commands</p>
</li>
<li><p>在其中创建 crawlall.py 文件 （<strong>此处文件名就是自定义的命令</strong>）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.commands <span class="keyword">import</span> ScrapyCommand</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Command</span><span class="params">(ScrapyCommand)</span>:</span></span><br><span class="line">	requires_project = <span class="literal">True</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">syntax</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">'[options]'</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">short_desc</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">'Runs all of the spiders'</span> <span class="comment">#使用--help查看命令的时候显示的信息</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, args, opts)</span>:</span></span><br><span class="line">		spider_list = self.crawler_process.spiders.list() <span class="comment">#找到所有的spider名称</span></span><br><span class="line">		<span class="keyword">for</span> name <span class="keyword">in</span> spider_list:</span><br><span class="line">			<span class="comment">#执行爬虫就绪命令，注意还没有开始去爬取</span></span><br><span class="line">            self.crawler_process.crawl(name, **opts.__dict__) </span><br><span class="line">		self.crawler_process.start() <span class="comment">#执行该命令才真正执行爬取操作</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在settings.py 中添加配置 COMMANDS_MODULE = ‘项目名称.目录名称’</p>
</li>
<li><p>在项目目录执行命令：scrapy crawlall </p>
</li>
</ul>
<p>拓展：如果我们不想通过命令行执行命令，而是想在py环境中执行scrapy可以调用scrapy中的cmdline下的execute方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    execute([<span class="string">"scrapy"</span>,<span class="string">"github"</span>,<span class="string">"--nolog"</span>]) <span class="comment"># 每个参数用逗号分隔</span></span><br></pre></td></tr></table></figure>



<hr>
<h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>以上是个人学习之路，如有误，欢迎指正！<br><a href="https://www.cnblogs.com/wupeiqi/articles/6229292.html" target="_blank" rel="noopener">参考文献</a></p>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python%E7%88%AC%E8%99%AB/">python爬虫    </a><a class="post-meta__tags" href="/tags/scrapy/">scrapy    </a><a class="post-meta__tags" href="/tags/%E5%85%A5%E9%97%A8/">入门    </a></div><div class="post_share"><div class="social-share" data-image="/img/scrapy入门_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/11/01/Scrapy%E6%A1%86%E6%9E%B6%E4%B9%8B%E7%BC%93%E5%AD%98%E4%B8%8E%E4%B8%AD%E9%97%B4%E4%BB%B6/"><img class="prev_cover lozad" data-src="/img/scrapy入门_cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>Scrapy框架之缓存与中间件</span></div></a></div><div class="next-post pull-right"><a href="/2019/10/25/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5/"><img class="next_cover lozad" data-src="/img/topimg.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>python爬虫之如何实现并发</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/11/09/scrapy框架源码剖析/" title="scrapy框架进阶"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">scrapy框架进阶</div></a></div><div class="relatedPosts_item"><a href="/2019/11/01/Scrapy框架之代理/" title="Scrapy框架之代理与https访问"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">Scrapy框架之代理与https访问</div></a></div><div class="relatedPosts_item"><a href="/2019/11/01/Scrapy框架之缓存与中间件/" title="Scrapy框架之缓存与中间件"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">Scrapy框架之缓存与中间件</div></a></div><div class="relatedPosts_item"><a href="/2019/11/16/Tornado框架入门/" title="Tornado框架入门"><img class="relatedPosts_cover lozad"data-src="/img/tornado_cover.jpg"><div class="relatedPosts_title">Tornado框架入门</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="lv-container" data-id="city" data-uid="MTAyMC80NzI1MC8yMzc1MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div></div><footer style="background-image: url(/img/scrapy入门topimg.jpg)"><div id="footer"><div class="copyright">&copy;2019 - 2020 By Rhubarb C</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="nightshift fa fa-moon-o" id="nightshift" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script color="0,0,255" opacity="0.7" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/js/canvas-nest.js"></script><script src="/js/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();
</script></body></html>