<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>scrapy框架进阶 | RhubarbC的博客</title><meta name="description" content="通过剖析Scrapy框架的源码更深的了解scrapy的学习心得"><meta name="keywords" content="python爬虫,scrapy,源码,进阶"><meta name="author" content="Rhubarb C"><meta name="copyright" content="Rhubarb C"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yoursite.com/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="scrapy框架进阶"><meta name="twitter:description" content="通过剖析Scrapy框架的源码更深的了解scrapy的学习心得"><meta name="twitter:image" content="http://yoursite.com/img/scrapy入门_cover.jpg"><meta property="og:type" content="article"><meta property="og:title" content="scrapy框架进阶"><meta property="og:url" content="http://yoursite.com/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"><meta property="og:site_name" content="RhubarbC的博客"><meta property="og:description" content="通过剖析Scrapy框架的源码更深的了解scrapy的学习心得"><meta property="og:image" content="http://yoursite.com/img/scrapy入门_cover.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="next" title="Scrapy框架之代理与https访问" href="http://yoursite.com/2019/11/01/Scrapy%E6%A1%86%E6%9E%B6%E4%B9%8B%E4%BB%A3%E7%90%86/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://deehuang.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: {"languages":{"author":"作者: Rhubarb C","link":"链接: http://yoursite.com/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/","source":"来源: RhubarbC的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  copy_copyright_js: true
  
}</script></head><body><canvas class="fireworks"></canvas><div id="header"> <div id="page-header"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">RhubarbC的博客</a></span><i class="fa fa-bars fa-fw toggle-menu pull-right close" aria-hidden="true"></i><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lozad avatar_img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">7</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy源码流程简述"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">scrapy源码流程简述</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy源码剖析"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">scrapy源码剖析</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Twisted使用"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">Twisted使用</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#自定义Low版scrapy框架"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">自定义Low版scrapy框架</span></a></li></ol></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy源码流程简述"><span class="toc-number">1.</span> <span class="toc-text">scrapy源码流程简述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy源码剖析"><span class="toc-number">2.</span> <span class="toc-text">scrapy源码剖析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Twisted使用"><span class="toc-number">2.1.</span> <span class="toc-text">Twisted使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自定义Low版scrapy框架"><span class="toc-number">2.2.</span> <span class="toc-text">自定义Low版scrapy框架</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/scrapy入门topimg.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">scrapy框架进阶</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-11-09<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-11-14</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">python网络爬虫</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/Scrapy/">Scrapy</a></span><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">4.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 14 分钟</span><span class="post-meta__separator">|</span><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="scrapy源码流程简述"><a href="#scrapy源码流程简述" class="headerlink" title="scrapy源码流程简述"></a>scrapy源码流程简述</h1><p>剖析scrapy框架我们要有一个入口。不难想到，在执行scrapy程序的时候，我们最开始的动作就是通过命令行去启动爬虫<code>scrapy crawl 爬虫名</code>。<br>在上次我写的<a href="https://deehuang.github.io/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/" target="_blank" rel="noopener">《scrapy框架入门》</a>中，有提到一个“自定义scrapy命令”的知识点。其实这就是我们剖析源码的一个入口！ </p>
<p>回忆一下自定义命令的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from scrapy.commands import ScrapyCommand</span><br><span class="line">from scrapy.utils.project import get_project_settings</span><br><span class="line"></span><br><span class="line">class Command(ScrapyCommand):</span><br><span class="line">	requires_project = True</span><br><span class="line">	def syntax(self):</span><br><span class="line">		return &apos;[options]&apos;</span><br><span class="line">	def short_desc(self):</span><br><span class="line">		return &apos;Runs all of the spiders&apos; #使用--help查看命令的时候显示的信息</span><br><span class="line"></span><br><span class="line">	def run(self, args, opts):</span><br><span class="line">		# 框架的入口 spiders</span><br><span class="line">		spider_list = self.crawler_process.spiders.list() #找到所有的spider名称</span><br><span class="line">		for name in spider_list:</span><br><span class="line">			#执行爬虫就绪命令，注意还没有开始去爬取</span><br><span class="line">            self.crawler_process.crawl(name, **opts.__dict__) </span><br><span class="line">		self.crawler_process.start() #执行该命令才真正执行爬取操作</span><br></pre></td></tr></table></figure>

<p>通过代码不难看出，框架执行就是执行<code>run</code>方法，而这个方法中的操作都是基于我们父类中的<code>self.crawler_process</code>属性进行的。<br>可以看一下<code>self.crawler_process</code>是一个什么类型的东西<code>print(type(self.crawler_process))</code></p>
<p>运行程序后在执行日志中发现，它是<code>CrawlerProcess</code>类的一个对象，那我们就可以找到源码中的这个类，下面就可以通过对它的执行流程去探索scrapy源码的奥秘：</p>
<p>源码流程简述:</p>
<ol>
<li>执行CrawlerProcess构造方法</li>
<li>CrawlerProcess对象（含有配置文件）的spiders<ol>
<li>为每个爬虫创建Crawler</li>
<li>执行<code>d = Crawler.crawl(...)</code>    <code>d = addBoth(_done)</code></li>
<li><code>CrawlerProcess对象._active = {d,}</code>  # {}是一个集合</li>
</ol>
</li>
<li><code>dd = defer.DeferredList(self._active)</code><br><code>dd.addBoth(self._stop_reactor)</code>  #self._stop_reactor  ==&gt;  reactor.stop()</li>
<li>rector.run  </li>
</ol>
<p>我们知道scrapy是基于twisted异步网络库来处理网络通信的，其实上面的一系列过程，和我们使用twisted的流程是一模一样的!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="comment"># from scrapy.core.engine import ExecutionEngine</span></span><br><span class="line"><span class="comment"># twisted异步请求示例</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    print(args,kwargs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    reactor.stop()</span><br><span class="line"></span><br><span class="line"><span class="meta">@defer.inlineCallbacks</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">(url)</span>:</span></span><br><span class="line">    v = getPage(url.encode(<span class="string">'utf-8'</span>))</span><br><span class="line">    v.addCallback(download)</span><br><span class="line">    <span class="keyword">yield</span> v</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url_list = [</span><br><span class="line">        <span class="string">"http://www.baidu.com"</span>,</span><br><span class="line">        <span class="string">"http://www.bing.com"</span>,</span><br><span class="line">        <span class="string">"http://dig.chouti.com"</span>,</span><br><span class="line">    ]</span><br><span class="line">    li = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        ret = task(url)</span><br><span class="line">        li.append(ret)</span><br><span class="line"></span><br><span class="line">    all_task = defer.DeferredList(li)</span><br><span class="line">    all_task.addBoth(stop)</span><br><span class="line">    reactor.run()</span><br></pre></td></tr></table></figure>

<p>这里做点改变就和上面的流程十分相似！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    print(args,kwargs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    reactor.stop()</span><br><span class="line"></span><br><span class="line"><span class="meta">@defer.inlineCallbacks</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">(url)</span>:</span></span><br><span class="line">    v = getPage(url.encode(<span class="string">'utf-8'</span>))  <span class="comment"># 下载页面</span></span><br><span class="line">    v.addBoth(download)   <span class="comment"># 把原来的addCallback改成这个,他是为了执行回调</span></span><br><span class="line">    <span class="keyword">yield</span> v</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url_list = [</span><br><span class="line">        <span class="string">"http://www.baidu.com"</span>,</span><br><span class="line">        <span class="string">"http://www.bing.com"</span>,</span><br><span class="line">        <span class="string">"http://dig.chouti.com"</span>,</span><br><span class="line">    ]</span><br><span class="line">    _active = set() <span class="comment"># 列表改为集合，放请求对象(即sock对象)</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        d = task(url)</span><br><span class="line">        _active.add(d)</span><br><span class="line"></span><br><span class="line">    dd = defer.DeferredList(_active) </span><br><span class="line">    dd.addBoth(stop)  <span class="comment"># stop ==&gt; reactor.stop</span></span><br><span class="line">    reactor.run()</span><br><span class="line">	<span class="comment"># 这是一个死循环，它会“一直”去DeferredList中监听所有的请求状态，如某一请求返回后接着从原来返回的函数往下执行（该实例中每个请求返回后去执行download）。所有请求处理完后触发d.addBoth中添加的回调函数，即all_done，实现异步操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求全部处理完后，reacotor.run还是会傻傻的一直循环，要终止循环我们需要在stop中去终止循环</span></span><br></pre></td></tr></table></figure>

<h1 id="scrapy源码剖析"><a href="#scrapy源码剖析" class="headerlink" title="scrapy源码剖析"></a>scrapy源码剖析</h1><p>我觉得学习应该是一套循序渐进的过程。在剖析源码前其实是需要做一些支持储备和时间以加深框架的理解的<br>我们知道scrapy是基于Twisted异步网络库来处理网络通信的。所以在剖析源码前，我们必须了解twisted的使用。然后根据我们使用scrapy的经验（在<a href="https://deehuang.github.io/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/" target="_blank" rel="noopener">入门阶段</a>学习框架每个组件的流程操作）再加上Twisted的功能，让我们来自己实现一个LOW版的scrapy框架来帮助我们去爬取网页，以此为过度来加深对scrapy框架的理解进而再去剖析srapy的源码。</p>
<h2 id="Twisted使用"><a href="#Twisted使用" class="headerlink" title="Twisted使用"></a>Twisted使用</h2><p>使用Twisted首先要导入三个模块：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from twisted.internet import reactor</span><br><span class="line">from twisted.web.client import getPage</span><br><span class="line">from twisted.internet import defer</span><br></pre></td></tr></table></figure>

<p>reactor：事件循环模块，可以理解为是select或epoll在一直死循环监听着socket对象的变化（终止条件：所有的socket都已经移除）</p>
<p>getPage：可以简单理解为用来创建socket对象发请求的一个模块（创建socket是为了模拟浏览器给服务端发送请求，发请求的目的是为了下载某一个页面），它会把根据ip+端口创建socket对象发起请求并放到reactor事件循环中监听，如果请求结束（链接成功\下载完成），自动从事件循环中移除</p>
<p>defer：<code>defer.Defferred</code>是一个特殊的socket对象。它不给任何地址发请求，即它不会有任何变化。如果把这个对象放在事件循环中的话就不会被移除掉而一直存在（因为socket有变化我们进行处理操作后就会移除该socket对象），所以事件循环就会一直监听。所以这个<code>Defferred</code>就是个伪造的socket对象来告诉事件循环不要停下来，如果要终止循环就需要我们人为手动的从事件循环中去移除这个socket对象</p>
<p>接下来就写一个使用<em>Twisted</em>来下载网页的案例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.利用getPage创建socket</span></span><br><span class="line"><span class="comment"># 2.将socket添加到事件循环中</span></span><br><span class="line"><span class="comment"># 3.开始事件循环（内部发送请求，并接受响应，当所有的socket请求完成后，终止事件循环）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># *********************利用getPage创建socket******************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">response</span><span class="params">(content)</span>:</span></span><br><span class="line">    print(content)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url = url.encode(<span class="string">'utf-8'</span>) <span class="comment"># Url需要是字节类型才能被解析</span></span><br><span class="line">    <span class="comment">#自动帮我们构造socket对象（通过dns解析会找到url对应的ip地址然后创建socket发请求）</span></span><br><span class="line">    d = getPage(url)  <span class="comment"># 只是创建s并conn，没有添加拿到响应后的回调函数，sock也没有放到事件循环中</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 响应回调</span></span><br><span class="line">    d.addCallback(response) <span class="comment"># 表示当页面下载完成后，自动调用方法中的回调函数response</span></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment"># *********************将socket添加到事件循环中******************</span></span><br><span class="line"><span class="meta">@defer.inlineCallbacks  # 需要加上装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url = url.encode(<span class="string">'utf-8'</span>) <span class="comment"># Url需要是字节类型才能被解析</span></span><br><span class="line">    <span class="comment">#自动帮我们构造socket对象（通过dns解析会找到url对应的ip地址然后创建socket发请求）</span></span><br><span class="line">    d = getPage(url)  <span class="comment"># 只是创建s并conn，没有添加拿到响应后的回调函数，sock也没有放到事件循环中</span></span><br><span class="line">    d.addCallback(response) <span class="comment"># 表示当页面下载完成后，自动调用方法中的回调函数response</span></span><br><span class="line">    <span class="keyword">yield</span> d  <span class="comment"># 装饰器+yield就成功把socket添加到事件循环中(select)了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># *********************开始事件循环******************</span></span><br><span class="line">d = task() <span class="comment"># 要先把生成器执行一下！！！否则函数内部是不执行的</span></span><br><span class="line">reactor.run() <span class="comment"># 内部就把事件循环开启了起来</span></span><br></pre></td></tr></table></figure>

<p>上面的代码有一个问题：事件循环会一直处于循环监听状态，因为缺少了终止条件去自动终止！<br>自动终止事件循环应该得有“一个东西”来监听或检测着事件循环的列表，到达某一程度条件的时候去终止</p>
<p>使用twisted的代码中，我们强调了要把“任务生成器”执行一边，我们会拿到一个可以认为是“<em>socket</em>”对象的返回值。<br>拿到它<code>d = task()</code>我们应该把它放在“某个东西里面”去监听起来，如果该请求对象返回响应并执行回调后，我们应该去终止循环。这个“东西”就是<code>dd=defer.DeferredList([d,])</code>通过下面伪代码会更好理解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">response</span><span class="params">(content)</span>:</span></span><br><span class="line">    print(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@defer.inlineCallbacks  # 需要加上装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""创建socket对象放到事件循环"""</span></span><br><span class="line">    url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url = url.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    d = getPage(url) </span><br><span class="line">    d.addCallback(response)</span><br><span class="line">    <span class="keyword">yield</span> d  <span class="comment"># 装饰器+yield就成功把socket添加到时间循环中了</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">done</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    reacor.stop</span><br><span class="line">    </span><br><span class="line">d = task() <span class="comment"># 要先把生成器执行一下！！！否则函数内部是不执行的</span></span><br><span class="line">dd = defer.DeferredList([d,]) <span class="comment"># 事件循环监听，监听对象是否完成回调</span></span><br><span class="line">dd.addBoth(done) <span class="comment"># 如果监听的dd中所有的d完成或失败后会调用addBoth中的函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">addCallback(func):表示所有的d完成回调后触发方法中的函数</span></span><br><span class="line"><span class="string">addErrback(func)：表示所有的d执行回调错误后触发方法中的函数</span></span><br><span class="line"><span class="string">addBoth(func): 表示无论所有d完成或失败回调出发方法中的函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">reactor.run() <span class="comment"># 内部就把事件循环开启了起来</span></span><br></pre></td></tr></table></figure>

<p>twisted可以实现并发发请求，他有两种方式可以实现：</p>
<p><strong>方式一</strong>：</p>
<p><img alt data-src="/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/4.jpg" class="lozad"></p>
<p>这里for循环去执行task不会因为每次去创建Socket发一个请求而卡住，因为twisted是异步IO模块，它是一次把所有socket对象创建完放到事件循环列表里面了（用的是非阻塞的socket所以不会阻塞）</p>
<p>要注意的是：task中只是创建了socket对象并且帮我们connet服务器。然后就把这个sock对象放到事件List中去监听了，这过程是不会阻塞的！而这些事情都是<em>@def.inlineCallback</em>里面帮我们去实现的</p>
<p><strong>方式二</strong>：</p>
<p>还记得我们开始介绍twisted的时候说<code>defer.Defferred</code>是一个特殊的sock对象，它不会只是创建的socket对象但是没有任何的连接conn请求操作，所以会导致整个事件循环的无法自动终止（自动终止的条件是当监听列表中的sock执行回调后触发终止函数）</p>
<p>第二种方式就是利用<code>defer.Defferred</code>去实现的，这种方式实现并发请求操作是需要我们手动的去终止循环的。手动实现检测请求是否返回处理可以利用计数器去实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"></span><br><span class="line">_close = <span class="literal">None</span></span><br><span class="line">count = <span class="number">0</span> <span class="comment"># 计数器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">response</span><span class="params">(content)</span>:</span></span><br><span class="line">    print(content)</span><br><span class="line">    <span class="keyword">global</span> count</span><br><span class="line">    count += <span class="number">1</span> <span class="comment"># 发出去的请求返回后加1，发出去请求数=计数器的时候关闭循环</span></span><br><span class="line">   	<span class="keyword">if</span> count == <span class="number">3</span>:</span><br><span class="line">        _close.callback(<span class="literal">None</span>) <span class="comment"># 这就能让Deferredlist监听到该特殊sock改变，执行终止函数</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@defer.inlineCallbacks  # 需要加上装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">()</span>:</span></span><br><span class="line">    url1 = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url1 = url1.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    d = getPage(url1) </span><br><span class="line">    d.addCallback(response)</span><br><span class="line">    </span><br><span class="line">    url2 = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url2 = url2.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    d = getPage(url2) </span><br><span class="line">    d.addCallback(response)</span><br><span class="line">    </span><br><span class="line">    url3 = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url3 = url.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    d = getPage(url3) </span><br><span class="line">    d.addCallback(response)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">global</span> _close</span><br><span class="line">    _close = defer.Deferred() <span class="comment"># 为了手动终止循环使用全局变量，使得其他作用域也能操作该对象</span></span><br><span class="line">    <span class="keyword">yield</span> _close</span><br><span class="line">    <span class="comment"># 返回一个特殊的sock对象，该对象不会执行回调所以无法被Deferredlist监听改变实现自动终止</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">done</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    reacor.stop</span><br><span class="line">    </span><br><span class="line">d = task() <span class="comment"># 要先把生成器执行一下！！！否则函数内部是不执行的</span></span><br><span class="line">dd = defer.DeferredList([d,]) <span class="comment"># 事件循环终止列表，监听事件循环中的对象是否完成回调</span></span><br><span class="line">dd.addBoth(done) <span class="comment"># 如果监听的dd中所有的d完成或失败后会调用addBoth中的函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">addCallback(func):表示所有的d完成回调后触发方法中的函数</span></span><br><span class="line"><span class="string">addErrback(func)：表示所有的d执行回调错误后触发方法中的函数</span></span><br><span class="line"><span class="string">addBoth(func): 表示无论所有d完成或失败回调出发方法中的函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">reactor.run() <span class="comment"># 内部就把事件循环开启了起来</span></span><br></pre></td></tr></table></figure>

<p>实际上，上面的代码就是scrapy框架的原型！</p>
<p>task可以理解为每个爬虫的开始，url1，url2，url3就是starts_url</p>
<p>而执行task()就是去放出一个爬虫，如果要执行多个爬虫可以弄多个task() </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spider1 = task()</span><br><span class="line">spider2 = task()</span><br><span class="line">dd = defer.DeferredList([spider1,spderd2])</span><br></pre></td></tr></table></figure>

<p>这就相当于放出了两个爬虫，这两个爬虫是并发发出的，并且每个爬虫内部发送请求也是并发去发的。实现了双并发！</p>
<p>了解了一些twisted基本使用和工作原理后我们就可以基于它写一个low版的框架了！</p>
<h2 id="自定义Low版scrapy框架"><a href="#自定义Low版scrapy框架" class="headerlink" title="自定义Low版scrapy框架"></a>自定义Low版scrapy框架</h2><p>回一下scrapy中的spider类中的代码：</p>
<p><img alt data-src="/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/5.jpg" class="lozad"></p>
<p>可以看出request对象就是帮我们发请求且执行回调函数的！所以我们要模仿者自己去封装request对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Request</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    封装往URL，以及请求的回调函数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,url,callback)</span></span></span><br><span class="line">    self.url = url</span><br><span class="line">    self.callback = callback</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HttpResponse</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    封装要传给回调函数中的所有信息</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,content,request)</span>:</span></span><br><span class="line">        self.content = content</span><br><span class="line">        self.request = request</span><br><span class="line">        self.url = request.url</span><br><span class="line">        self.text = str(content,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaiduSpider</span><span class="params">(object)</span>:</span></span><br><span class="line">    name = <span class="string">'baidu'</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        start_url = [<span class="string">'https://www.baidu.com'</span>,]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> start_url:</span><br><span class="line">            <span class="keyword">yield</span> Request(url,self.parse) <span class="comment"># 返回的是一个生成器</span></span><br><span class="line">            </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response,content)</span>:</span></span><br><span class="line">        print(response)</span><br><span class="line">        <span class="comment"># 1.爬取完成后执行回调所以要从crawlling 移除该爬取的url</span></span><br><span class="line">        <span class="comment"># 2.获取parse中yield的返回值</span></span><br><span class="line">        <span class="comment"># 3.再次去队列中获取</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">import</span> queue  </span><br><span class="line">Q = queue.Queue()  <span class="comment"># 调度器队列</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Engine</span><span class="params">(object)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._close = <span class="literal">None</span></span><br><span class="line">        self.max = <span class="number">5</span> <span class="comment"># 最大并发数</span></span><br><span class="line">        self.crawlling = []  <span class="comment"># 正在爬取的url列表</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_response_callback</span><span class="params">(self，content,request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        在回调函数中我们要去完成三件事：</span></span><br><span class="line"><span class="string">        1.爬取完成后执行回调所以要从crawlling 移除该爬取的url</span></span><br><span class="line"><span class="string">        2.获取parse中yield的返回值</span></span><br><span class="line"><span class="string">        3.再次去队列中获取</span></span><br><span class="line"><span class="string">        而scrapy的使用中我们直接通过yield就完成了所有操作，所以在callback中传入的不应该是用户自定义的parse方法，而是我们做了修饰的方法，这里就有点像是一个装饰器。十分有灵性</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.crawlling.remove(request)</span><br><span class="line">        rep = HttpResponse(content,request)</span><br><span class="line">        <span class="comment"># 执行下回调，十分类似装饰器的效果。函数如果有返回值拿到的可能会是一个生成器对象</span></span><br><span class="line">        result = request.callback(rep) </span><br><span class="line">        <span class="keyword">import</span> types</span><br><span class="line">        <span class="keyword">if</span> isinstance(result,types.GeneratorType):</span><br><span class="line">            <span class="comment"># 如果是一个生成器就把每一个yield的request对象都放到调度器队列中（这里是只判断yield Request对象的情况）</span></span><br><span class="line">            <span class="keyword">for</span> req <span class="keyword">in</span> result:</span><br><span class="line">                Q.put(req)</span><br><span class="line">                </span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_next_request</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        去调度器中取request，并发送请求 （有最大并发数的限制） </span></span><br><span class="line"><span class="string">        由于【反复】取url得操作会被频繁使用，所以封装成方法</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> Q.qsize() == <span class="number">0</span> <span class="keyword">and</span> len(self.crawlling) == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 当调度器和正在爬取的列表都为空的时候表示所有的请求都已经处理完了</span></span><br><span class="line">            self._close.callback(<span class="literal">None</span>) <span class="comment"># 手动移除特殊sock对象</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> len(self.cralling) &gt;= self.max: <span class="comment"># 如果正在爬取的url个数大于并发数，就不去取了</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    	<span class="keyword">while</span> len(self.crawling) &lt; self.max:</span><br><span class="line">            <span class="comment"># 当正在爬取url列表中个数小于最大并发数的时候，取url补充到列表中：去发送请求，执回调</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">            	req = Q.get(block=<span class="literal">False</span>) <span class="comment"># 让队列为空的时候不阻塞，但是会报错</span></span><br><span class="line">                self.crawlling.append(req)</span><br><span class="line">                d = getPage(req.url.encode(<span class="string">'utf-8'</span>)) <span class="comment">#发送请求</span></span><br><span class="line">                <span class="comment">#页面下载完成调用get_response_callback.</span></span><br><span class="line">                <span class="comment">#该方法里面调用用户spider中定义的回调方法并且将新请求添加到调度器</span></span><br><span class="line">                d.addCallback(self.get_response_callback,req)<span class="comment">#把请求信息也放入回调函数中</span></span><br><span class="line">                <span class="comment">#未移除正在爬取的url前会到达最大并发退出循环，所以需要再加一个回调去做爬取操作</span></span><br><span class="line">                <span class="comment">#执行完上面的回调后再执行一个回调（双回调）</span></span><br><span class="line">                <span class="comment">#未达到达到最大并发数，可以再去调度器中获取Requst</span></span><br><span class="line">                d.addCallback(<span class="keyword">lambda</span> _:reactor.callLater(<span class="number">0</span>,self._next_request))</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="comment">#调度器中没有requst了就返回</span></span><br><span class="line">                <span class="keyword">return</span> </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line"><span class="meta">    @defer.inlineCallbacks</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        <span class="string">"""进行下载操作（发请求--&gt;放事件监听--&gt;得响应执回调）"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#将初始Request对象添加到调度器</span></span><br><span class="line">        </span><br><span class="line">      	<span class="comment">#拿到起始请求对象,迭代器转换成生成器可以通过next取值</span></span><br><span class="line">        start_requests = iter(spider.start_requests())</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 多个起始请求对象需要进行循环next取到放到队列中</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                request = next(start_requests) <span class="comment">#拿到起始请求对象（封装到Request中）</span></span><br><span class="line">                Q.put(requst) <span class="comment">#把请求放到队列中，这队列就是我们的调度器</span></span><br><span class="line">            <span class="keyword">except</span> StopIteration <span class="keyword">as</span> e:</span><br><span class="line">                <span class="comment"># 迭代器遍历了所有元素后会抛出异常</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">              </span><br><span class="line">        	<span class="comment"># 【反复】的去调度器中取任务下载，下载完成后执行回调函数</span></span><br><span class="line">			<span class="comment">#self._next_request() # 去调度器中取request，并发送请求  </span></span><br><span class="line">            reactor.callLater(<span class="number">0</span>,self._next_request) <span class="comment">#事件循环来帮我们调用回调（可设置时间）</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        self.spider = spider</span><br><span class="line">        self._close = defer.Deferred()</span><br><span class="line">        <span class="keyword">yield</span> self._close</span><br><span class="line"></span><br><span class="line">spider = BaiduSpider() <span class="comment"># 创建爬虫对象        </span></span><br><span class="line">        </span><br><span class="line">_active = set()</span><br><span class="line">engine = Engine() <span class="comment"># 创建引擎</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">d = engine.crawl(spider) <span class="comment"># 执行爬取操作</span></span><br><span class="line">_active.add(d)</span><br><span class="line"></span><br><span class="line">dd = defer.DeferredList(_active)</span><br><span class="line">dd.addBoth(<span class="keyword">lambda</span> _:reactor.stop()) <span class="comment"># lambda本身就是个函数</span></span><br><span class="line"></span><br><span class="line">reactor.run()</span><br></pre></td></tr></table></figure>

<p>主要用到的知识点有：</p>
<p><img alt data-src="/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/6.jpg" class="lozad"></p>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python%E7%88%AC%E8%99%AB/">python爬虫    </a><a class="post-meta__tags" href="/tags/scrapy/">scrapy    </a><a class="post-meta__tags" href="/tags/%E6%BA%90%E7%A0%81/">源码    </a><a class="post-meta__tags" href="/tags/%E8%BF%9B%E9%98%B6/">进阶    </a></div><div class="post_share"><div class="social-share" data-image="/img/scrapy入门_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/11/01/Scrapy%E6%A1%86%E6%9E%B6%E4%B9%8B%E4%BB%A3%E7%90%86/"><img class="next_cover lozad" data-src="/img/scrapy入门_cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Scrapy框架之代理与https访问</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/10/30/Scrapy框架/" title="Scrapy框架入门"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">Scrapy框架入门</div></a></div><div class="relatedPosts_item"><a href="/2019/11/01/Scrapy框架之代理/" title="Scrapy框架之代理与https访问"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">Scrapy框架之代理与https访问</div></a></div><div class="relatedPosts_item"><a href="/2019/11/01/Scrapy框架之缓存与中间件/" title="Scrapy框架之缓存与中间件"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">Scrapy框架之缓存与中间件</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="lv-container" data-id="city" data-uid="MTAyMC80NzI1MC8yMzc1MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div></div><footer style="background-image: url(/img/scrapy入门topimg.jpg)"><div id="footer"><div class="copyright">&copy;2019 By Rhubarb C</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="nightshift fa fa-moon-o" id="nightshift" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script color="0,0,255" opacity="0.7" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/js/canvas-nest.js"></script><script src="/js/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();
</script></body></html>