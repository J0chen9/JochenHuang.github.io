<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>scrapy框架进阶 | RhubarbC的博客</title><meta name="description" content="通过剖析Scrapy框架的源码更深的了解scrapy的学习心得"><meta name="keywords" content="python爬虫,scrapy,进阶"><meta name="author" content="Rhubarb C"><meta name="copyright" content="Rhubarb C"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yoursite.com/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="scrapy框架进阶"><meta name="twitter:description" content="通过剖析Scrapy框架的源码更深的了解scrapy的学习心得"><meta name="twitter:image" content="http://yoursite.com/img/scrapy入门_cover.jpg"><meta property="og:type" content="article"><meta property="og:title" content="scrapy框架进阶"><meta property="og:url" content="http://yoursite.com/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"><meta property="og:site_name" content="RhubarbC的博客"><meta property="og:description" content="通过剖析Scrapy框架的源码更深的了解scrapy的学习心得"><meta property="og:image" content="http://yoursite.com/img/scrapy入门_cover.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="Tornado框架入门" href="http://yoursite.com/2019/11/16/Tornado%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8/"><link rel="next" title="Scrapy框架之代理与https访问" href="http://yoursite.com/2019/11/01/Scrapy%E6%A1%86%E6%9E%B6%E4%B9%8B%E4%BB%A3%E7%90%86/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://deehuang.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  copyright: {"languages":{"author":"作者: Rhubarb C","link":"链接: http://yoursite.com/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/","source":"来源: RhubarbC的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  copy_copyright_js: true
  
}</script></head><body><canvas class="fireworks"></canvas><div id="header"> <div id="page-header"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">RhubarbC的博客</a></span><i class="fa fa-bars fa-fw toggle-menu pull-right close" aria-hidden="true"></i><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lozad avatar_img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">19</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">33</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">11</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><script>document.body.addEventListener('touchstart', function(){ });</script></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy源码流程简述"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">scrapy源码流程简述</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#scrapy源码剖析"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">scrapy源码剖析</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Twisted使用"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">Twisted使用</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#自定义Low版scrapy框架"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">自定义Low版scrapy框架</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#结语"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">结语</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy源码流程简述"><span class="toc-number">1.</span> <span class="toc-text">scrapy源码流程简述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy源码剖析"><span class="toc-number">2.</span> <span class="toc-text">scrapy源码剖析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Twisted使用"><span class="toc-number">2.1.</span> <span class="toc-text">Twisted使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自定义Low版scrapy框架"><span class="toc-number">2.2.</span> <span class="toc-text">自定义Low版scrapy框架</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#结语"><span class="toc-number">3.</span> <span class="toc-text">结语</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/scrapy入门topimg.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">scrapy框架进阶</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-11-09<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-11-16</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/">python网络爬虫</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/Scrapy/">Scrapy</a></span><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">5.8k</span><span class="post-meta__separator">|</span><span>阅读时长: 21 分钟</span><span class="post-meta__separator">|</span><span>阅读量: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="scrapy源码流程简述"><a href="#scrapy源码流程简述" class="headerlink" title="scrapy源码流程简述"></a>scrapy源码流程简述</h1><p>剖析scrapy框架我们要有一个入口。不难想到，在执行scrapy程序的时候，我们最开始的动作就是通过命令行去启动爬虫<code>scrapy crawl 爬虫名</code>。<br>在上次我写的<a href="https://deehuang.github.io/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/" target="_blank" rel="noopener">《scrapy框架入门》</a>中，有提到一个“自定义scrapy命令”的知识点。其实这就是我们剖析源码的一个入口！ </p>
<p>回忆一下自定义命令的代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from scrapy.commands import ScrapyCommand</span><br><span class="line">from scrapy.utils.project import get_project_settings</span><br><span class="line"></span><br><span class="line">class Command(ScrapyCommand):</span><br><span class="line">	requires_project = True</span><br><span class="line">	def syntax(self):</span><br><span class="line">		return &apos;[options]&apos;</span><br><span class="line">	def short_desc(self):</span><br><span class="line">		return &apos;Runs all of the spiders&apos; #使用--help查看命令的时候显示的信息</span><br><span class="line"></span><br><span class="line">	def run(self, args, opts):</span><br><span class="line">		# 框架的入口 spiders</span><br><span class="line">		spider_list = self.crawler_process.spiders.list() #找到所有的spider名称</span><br><span class="line">		for name in spider_list:</span><br><span class="line">			#执行爬虫就绪命令，注意还没有开始去爬取</span><br><span class="line">            self.crawler_process.crawl(name, **opts.__dict__) </span><br><span class="line">		self.crawler_process.start() #执行该命令才真正执行爬取操作</span><br></pre></td></tr></table></figure>

<p>通过代码不难看出，框架执行就是执行<code>run</code>方法，而这个方法中的操作都是基于我们父类中的<code>self.crawler_process</code>属性进行的。<br>可以看一下<code>self.crawler_process</code>是一个什么类型的东西<code>print(type(self.crawler_process))</code></p>
<p>运行程序后在执行日志中发现，它是<code>CrawlerProcess</code>类的一个对象，那我们就可以找到源码中的这个类，下面就可以通过对它的执行流程去探索scrapy源码的奥秘：</p>
<p>源码流程简述:</p>
<ol>
<li>执行CrawlerProcess构造方法</li>
<li>CrawlerProcess对象（含有配置文件）的spiders<ol>
<li>为每个爬虫创建Crawler</li>
<li>执行<code>d = Crawler.crawl(...)</code>    <code>d = addBoth(_done)</code></li>
<li><code>CrawlerProcess对象._active = {d,}</code>  # {}是一个集合</li>
</ol>
</li>
<li><code>dd = defer.DeferredList(self._active)</code><br><code>dd.addBoth(self._stop_reactor)</code>  #self._stop_reactor  ==&gt;  reactor.stop()</li>
<li>rector.run  </li>
</ol>
<p>我们知道scrapy是基于twisted异步网络库来处理网络通信的，其实上面的一系列过程，和我们使用twisted的流程是一模一样的!</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="comment"># from scrapy.core.engine import ExecutionEngine</span></span><br><span class="line"><span class="comment"># twisted异步请求示例</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    print(args,kwargs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    reactor.stop()</span><br><span class="line"></span><br><span class="line"><span class="meta">@defer.inlineCallbacks</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">(url)</span>:</span></span><br><span class="line">    v = getPage(url.encode(<span class="string">'utf-8'</span>))</span><br><span class="line">    v.addCallback(download)</span><br><span class="line">    <span class="keyword">yield</span> v</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url_list = [</span><br><span class="line">        <span class="string">"http://www.baidu.com"</span>,</span><br><span class="line">        <span class="string">"http://www.bing.com"</span>,</span><br><span class="line">        <span class="string">"http://dig.chouti.com"</span>,</span><br><span class="line">    ]</span><br><span class="line">    li = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        ret = task(url)</span><br><span class="line">        li.append(ret)</span><br><span class="line"></span><br><span class="line">    all_task = defer.DeferredList(li)</span><br><span class="line">    all_task.addBoth(stop)</span><br><span class="line">    reactor.run()</span><br></pre></td></tr></table></figure>

<p>这里做点改变就和上面的流程十分相似！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    print(args,kwargs)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    reactor.stop()</span><br><span class="line"></span><br><span class="line"><span class="meta">@defer.inlineCallbacks</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">(url)</span>:</span></span><br><span class="line">    v = getPage(url.encode(<span class="string">'utf-8'</span>))  <span class="comment"># 下载页面</span></span><br><span class="line">    v.addBoth(download)   <span class="comment"># 把原来的addCallback改成这个,他是为了执行回调</span></span><br><span class="line">    <span class="keyword">yield</span> v</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    url_list = [</span><br><span class="line">        <span class="string">"http://www.baidu.com"</span>,</span><br><span class="line">        <span class="string">"http://www.bing.com"</span>,</span><br><span class="line">        <span class="string">"http://dig.chouti.com"</span>,</span><br><span class="line">    ]</span><br><span class="line">    _active = set() <span class="comment"># 列表改为集合，放请求对象(即sock对象)</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">        d = task(url)</span><br><span class="line">        _active.add(d)</span><br><span class="line"></span><br><span class="line">    dd = defer.DeferredList(_active) </span><br><span class="line">    dd.addBoth(stop)  <span class="comment"># stop ==&gt; reactor.stop</span></span><br><span class="line">    reactor.run()</span><br><span class="line">	<span class="comment"># 这是一个死循环，它会“一直”去DeferredList中监听所有的请求状态，如某一请求返回后接着从原来返回的函数往下执行（该实例中每个请求返回后去执行download）。所有请求处理完后触发d.addBoth中添加的回调函数，即all_done，实现异步操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求全部处理完后，reacotor.run还是会傻傻的一直循环，要终止循环我们需要在stop中去终止循环</span></span><br></pre></td></tr></table></figure>

<h1 id="scrapy源码剖析"><a href="#scrapy源码剖析" class="headerlink" title="scrapy源码剖析"></a>scrapy源码剖析</h1><p>我觉得学习应该是一套循序渐进的过程。在剖析源码前其实是需要做一些支持储备和时间以加深框架的理解的<br>我们知道scrapy是基于Twisted异步网络库来处理网络通信的。所以在剖析源码前，我们必须了解twisted的使用。然后根据我们使用scrapy的经验（在<a href="https://deehuang.github.io/2019/10/30/Scrapy%E6%A1%86%E6%9E%B6/" target="_blank" rel="noopener">入门阶段</a>学习框架每个组件的流程操作）再加上Twisted的功能，让我们来自己实现一个LOW版的scrapy框架来帮助我们去爬取网页，以此为过度来加深对scrapy框架的理解进而再去剖析scrapy的源码。</p>
<h2 id="Twisted使用"><a href="#Twisted使用" class="headerlink" title="Twisted使用"></a>Twisted使用</h2><p>使用Twisted首先要导入三个模块：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from twisted.internet import reactor</span><br><span class="line">from twisted.web.client import getPage</span><br><span class="line">from twisted.internet import defer</span><br></pre></td></tr></table></figure>

<p>reactor：事件循环模块，可以理解为是select或epoll在一直死循环监听着socket对象的变化（终止条件：所有的socket都已经移除）</p>
<p>getPage：可以简单理解为用来创建socket对象发请求的一个模块（创建socket是为了模拟浏览器给服务端发送请求，发请求的目的是为了下载某一个页面），它会把根据ip+端口创建socket对象发起请求并放到reactor事件循环中监听，如果请求结束（链接成功\下载完成），自动从事件循环中移除</p>
<p>defer：<code>defer.Defferred</code>是一个特殊的socket对象。它不给任何地址发请求，即它不会有任何变化。如果把这个对象放在事件循环中的话就不会被移除掉而一直存在（因为socket有变化我们进行处理操作后就会移除该socket对象），所以事件循环就会一直监听。所以这个<code>Defferred</code>就是个伪造的socket对象来告诉事件循环不要停下来，如果要终止循环就需要我们人为手动的从事件循环中去移除这个socket对象</p>
<p>接下来就写一个使用<em>Twisted</em>来下载网页的案例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.利用getPage创建socket</span></span><br><span class="line"><span class="comment"># 2.将socket添加到事件循环中</span></span><br><span class="line"><span class="comment"># 3.开始事件循环（内部发送请求，并接受响应，当所有的socket请求完成后，终止事件循环）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># *********************利用getPage创建socket******************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">response</span><span class="params">(content)</span>:</span></span><br><span class="line">    print(content)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url = url.encode(<span class="string">'utf-8'</span>) <span class="comment"># Url需要是字节类型才能被解析</span></span><br><span class="line">    <span class="comment">#自动帮我们构造socket对象（通过dns解析会找到url对应的ip地址然后创建socket发请求）</span></span><br><span class="line">    d = getPage(url)  <span class="comment"># 只是创建s并conn，没有添加拿到响应后的回调函数，sock也没有放到事件循环中</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 响应回调</span></span><br><span class="line">    d.addCallback(response) <span class="comment"># 表示当页面下载完成后，自动调用方法中的回调函数response</span></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment"># *********************将socket添加到事件循环中******************</span></span><br><span class="line"><span class="meta">@defer.inlineCallbacks  # 需要加上装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">()</span>:</span></span><br><span class="line">    url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url = url.encode(<span class="string">'utf-8'</span>) <span class="comment"># Url需要是字节类型才能被解析</span></span><br><span class="line">    <span class="comment">#自动帮我们构造socket对象（通过dns解析会找到url对应的ip地址然后创建socket发请求）</span></span><br><span class="line">    d = getPage(url)  <span class="comment"># 只是创建s并conn，没有添加拿到响应后的回调函数，sock也没有放到事件循环中</span></span><br><span class="line">    d.addCallback(response) <span class="comment"># 表示当页面下载完成后，自动调用方法中的回调函数response</span></span><br><span class="line">    <span class="keyword">yield</span> d  <span class="comment"># 装饰器+yield就成功把socket添加到事件循环中(select)了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># *********************开始事件循环******************</span></span><br><span class="line">d = task() <span class="comment"># 要先把生成器执行一下！！！否则函数内部是不执行的</span></span><br><span class="line">reactor.run() <span class="comment"># 内部就把事件循环开启了起来</span></span><br></pre></td></tr></table></figure>

<p>上面的代码有一个问题：事件循环会一直处于循环监听状态，因为缺少了终止条件去自动终止！<br>自动终止事件循环应该得有“一个东西”来监听或检测着事件循环的列表，到达某一程度条件的时候去终止</p>
<p>使用twisted的代码中，我们强调了要把“任务生成器”执行一边，我们会拿到一个可以认为是“<em>socket</em>”对象的返回值。<br>拿到它<code>d = task()</code>我们应该把它放在“某个东西里面”去监听起来，如果该请求对象返回响应并执行回调后，我们应该去终止循环。这个“东西”就是<code>dd=defer.DeferredList([d,])</code>通过下面伪代码会更好理解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">response</span><span class="params">(content)</span>:</span></span><br><span class="line">    print(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@defer.inlineCallbacks  # 需要加上装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""创建socket对象放到事件循环"""</span></span><br><span class="line">    url = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url = url.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    d = getPage(url) </span><br><span class="line">    d.addCallback(response)</span><br><span class="line">    <span class="keyword">yield</span> d  <span class="comment"># 装饰器+yield就成功把socket添加到时间循环中了</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">done</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    reacor.stop</span><br><span class="line">    </span><br><span class="line">d = task() <span class="comment"># 要先把生成器执行一下！！！否则函数内部是不执行的</span></span><br><span class="line">dd = defer.DeferredList([d,]) <span class="comment"># 事件循环监听，监听对象是否完成回调</span></span><br><span class="line">dd.addBoth(done) <span class="comment"># 如果监听的dd中所有的d完成或失败后会调用addBoth中的函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">addCallback(func):表示所有的d完成回调后触发方法中的函数</span></span><br><span class="line"><span class="string">addErrback(func)：表示所有的d执行回调错误后触发方法中的函数</span></span><br><span class="line"><span class="string">addBoth(func): 表示无论所有d完成或失败回调出发方法中的函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">reactor.run() <span class="comment"># 内部就把事件循环开启了起来</span></span><br></pre></td></tr></table></figure>

<p>twisted可以实现并发发请求，他有两种方式可以实现：</p>
<p><strong>方式一</strong>：</p>
<p><img alt data-src="/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/4.jpg" class="lozad"></p>
<p>这里for循环去执行task不会因为每次去创建Socket发一个请求而卡住，因为twisted是异步IO模块，它是一次把所有socket对象创建完放到事件循环列表里面了（用的是非阻塞的socket所以不会阻塞）</p>
<p>要注意的是：task中只是创建了socket对象并且帮我们connet服务器。然后就把这个sock对象放到事件List中去监听了，这过程是不会阻塞的！而这些事情都是<em>@def.inlineCallback</em>里面帮我们去实现的</p>
<p><strong>方式二</strong>：</p>
<p>还记得我们开始介绍twisted的时候说<code>defer.Defferred</code>是一个特殊的sock对象，它不会只是创建的socket对象但是没有任何的连接conn请求操作，所以会导致整个事件循环的无法自动终止（自动终止的条件是当监听列表中的sock执行回调后触发终止函数）</p>
<p>第二种方式就是利用<code>defer.Defferred</code>去实现的，这种方式实现并发请求操作是需要我们手动的去终止循环的。手动实现检测请求是否返回处理可以利用计数器去实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"></span><br><span class="line">_close = <span class="literal">None</span></span><br><span class="line">count = <span class="number">0</span> <span class="comment"># 计数器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">response</span><span class="params">(content)</span>:</span></span><br><span class="line">    print(content)</span><br><span class="line">    <span class="keyword">global</span> count</span><br><span class="line">    count += <span class="number">1</span> <span class="comment"># 发出去的请求返回后加1，发出去请求数=计数器的时候关闭循环</span></span><br><span class="line">   	<span class="keyword">if</span> count == <span class="number">3</span>:</span><br><span class="line">        _close.callback(<span class="literal">None</span>) <span class="comment"># 这就能让Deferredlist监听到该特殊sock改变，执行终止函数</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@defer.inlineCallbacks  # 需要加上装饰器</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">()</span>:</span></span><br><span class="line">    url1 = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url1 = url1.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    d = getPage(url1) </span><br><span class="line">    d.addCallback(response)</span><br><span class="line">    </span><br><span class="line">    url2 = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url2 = url2.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    d = getPage(url2) </span><br><span class="line">    d.addCallback(response)</span><br><span class="line">    </span><br><span class="line">    url3 = <span class="string">'http://www.baidu.com'</span></span><br><span class="line">    url3 = url.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    d = getPage(url3) </span><br><span class="line">    d.addCallback(response)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">global</span> _close</span><br><span class="line">    _close = defer.Deferred() <span class="comment"># 为了手动终止循环使用全局变量，使得其他作用域也能操作该对象</span></span><br><span class="line">    <span class="keyword">yield</span> _close</span><br><span class="line">    <span class="comment"># 返回一个特殊的sock对象，该对象不会执行回调所以无法被Deferredlist监听改变实现自动终止</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">done</span><span class="params">(*args,**kwargs)</span>:</span></span><br><span class="line">    reacor.stop</span><br><span class="line">    </span><br><span class="line">d = task() <span class="comment"># 要先把生成器执行一下！！！否则函数内部是不执行的</span></span><br><span class="line">dd = defer.DeferredList([d,]) <span class="comment"># 事件循环终止列表，监听事件循环中的对象是否完成回调</span></span><br><span class="line">dd.addBoth(done) <span class="comment"># 如果监听的dd中所有的d完成或失败后会调用addBoth中的函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">addCallback(func):表示所有的d完成回调后触发方法中的函数</span></span><br><span class="line"><span class="string">addErrback(func)：表示所有的d执行回调错误后触发方法中的函数</span></span><br><span class="line"><span class="string">addBoth(func): 表示无论所有d完成或失败回调出发方法中的函数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">reactor.run() <span class="comment"># 内部就把事件循环开启了起来</span></span><br></pre></td></tr></table></figure>

<p>实际上，上面的代码就是scrapy框架的原型！</p>
<p>task可以理解为每个爬虫的开始，url1，url2，url3就是starts_url</p>
<p>而执行task()就是去放出一个爬虫，如果要执行多个爬虫可以弄多个task() </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spider1 = task()</span><br><span class="line">spider2 = task()</span><br><span class="line">dd = defer.DeferredList([spider1,spderd2])</span><br></pre></td></tr></table></figure>

<p>这就相当于放出了两个爬虫，这两个爬虫是并发发出的，并且每个爬虫内部发送请求也是并发去发的。实现了双并发！</p>
<p>了解了一些twisted基本使用和工作原理后我们就可以基于它写一个low版的框架了！</p>
<h2 id="自定义Low版scrapy框架"><a href="#自定义Low版scrapy框架" class="headerlink" title="自定义Low版scrapy框架"></a>自定义Low版scrapy框架</h2><p>回一下scrapy中的spider类中的代码：</p>
<p><img alt data-src="/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/5.jpg" class="lozad"></p>
<p>可以看出request对象就是帮我们发请求且执行回调函数的！所以我们要模仿着自己去封装request对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Request</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    封装往URL，以及请求的回调函数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,url,callback)</span></span></span><br><span class="line">    self.url = url</span><br><span class="line">    self.callback = callback</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HttpResponse</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    封装要传给回调函数中的所有信息</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,content,request)</span>:</span></span><br><span class="line">        self.content = content</span><br><span class="line">        self.request = request</span><br><span class="line">        self.url = request.url</span><br><span class="line">        self.text = str(content,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaiduSpider</span><span class="params">(object)</span>:</span></span><br><span class="line">    name = <span class="string">'baidu'</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        start_url = [<span class="string">'https://www.baidu.com'</span>,]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> start_url:</span><br><span class="line">            <span class="keyword">yield</span> Request(url,self.parse) <span class="comment"># 返回的是一个生成器</span></span><br><span class="line">            </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response,content)</span>:</span></span><br><span class="line">        print(response)</span><br><span class="line">        <span class="comment"># 1.爬取完成后执行回调所以要从crawlling 移除该爬取的url</span></span><br><span class="line">        <span class="comment"># 2.获取parse中yield的返回值</span></span><br><span class="line">        <span class="comment"># 3.再次去队列中获取</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">import</span> queue  </span><br><span class="line">Q = queue.Queue()  <span class="comment"># 调度器队列</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Engine</span><span class="params">(object)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._close = <span class="literal">None</span></span><br><span class="line">        self.max = <span class="number">5</span> <span class="comment"># 最大并发数</span></span><br><span class="line">        self.crawlling = []  <span class="comment"># 正在爬取的url列表</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_response_callback</span><span class="params">(self，content,request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        在回调函数中我们要去完成三件事：</span></span><br><span class="line"><span class="string">        1.爬取完成后执行回调所以要从crawlling 移除该爬取的url</span></span><br><span class="line"><span class="string">        2.获取parse中yield的返回值</span></span><br><span class="line"><span class="string">        3.再次去队列中获取</span></span><br><span class="line"><span class="string">        而scrapy的使用中我们直接通过yield就完成了所有操作，所以在callback中传入的不应该是用户自定义的parse方法，而是我们做了修饰的方法，这里就有点像是一个装饰器。十分有灵性</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.crawlling.remove(request)</span><br><span class="line">        rep = HttpResponse(content,request)</span><br><span class="line">        <span class="comment"># 执行下回调，十分类似装饰器的效果。函数如果有返回值拿到的可能会是一个生成器对象</span></span><br><span class="line">        result = request.callback(rep) </span><br><span class="line">        <span class="keyword">import</span> types</span><br><span class="line">        <span class="keyword">if</span> isinstance(result,types.GeneratorType):</span><br><span class="line">            <span class="comment"># 如果是一个生成器就把每一个yield的request对象都放到调度器队列中（这里是只判断yield Request对象的情况）</span></span><br><span class="line">            <span class="keyword">for</span> req <span class="keyword">in</span> result:</span><br><span class="line">                Q.put(req)</span><br><span class="line">                </span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_next_request</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        去调度器中取request，并发送请求 （有最大并发数的限制） </span></span><br><span class="line"><span class="string">        由于【反复】取url得操作会被频繁使用，所以封装成方法</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> Q.qsize() == <span class="number">0</span> <span class="keyword">and</span> len(self.crawlling) == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 当调度器和正在爬取的列表都为空的时候表示所有的请求都已经处理完了</span></span><br><span class="line">            self._close.callback(<span class="literal">None</span>) <span class="comment"># 手动移除特殊sock对象</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> len(self.cralling) &gt;= self.max: <span class="comment"># 如果正在爬取的url个数大于并发数，就不去取了</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    	<span class="keyword">while</span> len(self.crawling) &lt; self.max:</span><br><span class="line">            <span class="comment"># 当正在爬取url列表中个数小于最大并发数的时候，取url补充到列表中：去发送请求，执回调</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">            	req = Q.get(block=<span class="literal">False</span>) <span class="comment"># 让队列为空的时候不阻塞，但是会报错</span></span><br><span class="line">                self.crawlling.append(req)</span><br><span class="line">                d = getPage(req.url.encode(<span class="string">'utf-8'</span>)) <span class="comment">#发送请求</span></span><br><span class="line">                <span class="comment">#页面下载完成调用get_response_callback.</span></span><br><span class="line">                <span class="comment">#该方法里面调用用户spider中定义的回调方法并且将新请求添加到调度器</span></span><br><span class="line">                d.addCallback(self.get_response_callback,req)<span class="comment">#把请求信息也放入回调函数中</span></span><br><span class="line">                <span class="comment">#移除了正在爬取的url后还需要再去取url发请求，所以需要再加一个回调去做爬取操作</span></span><br><span class="line">                <span class="comment">#执行完上面的回调后再执行一个回调（双回调）</span></span><br><span class="line">                <span class="comment">#未达到达到最大并发数，可以再去调度器中获取Requst</span></span><br><span class="line">                d.addCallback(<span class="keyword">lambda</span> _:reactor.callLater(<span class="number">0</span>,self._next_request))</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="comment">#调度器中没有requst了就返回</span></span><br><span class="line">                <span class="keyword">return</span> </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line"><span class="meta">    @defer.inlineCallbacks</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        <span class="string">"""进行下载操作（发请求--&gt;放事件监听--&gt;得响应执回调）"""</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#将初始Request对象添加到调度器</span></span><br><span class="line">        </span><br><span class="line">      	<span class="comment">#拿到起始请求对象,迭代器转换成生成器可以通过next取值</span></span><br><span class="line">        start_requests = iter(spider.start_requests())</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 多个起始请求对象需要进行循环next取到放到队列中</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                request = next(start_requests) <span class="comment">#拿到起始请求对象（封装到Request中）</span></span><br><span class="line">                Q.put(requst) <span class="comment">#把请求放到队列中，这队列就是我们的调度器</span></span><br><span class="line">            <span class="keyword">except</span> StopIteration <span class="keyword">as</span> e:</span><br><span class="line">                <span class="comment"># 迭代器遍历了所有元素后会抛出异常</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">              </span><br><span class="line">        	<span class="comment"># 【反复】的去调度器中取任务下载，下载完成后执行回调函数</span></span><br><span class="line">			<span class="comment">#self._next_request() # 去调度器中取request，并发送请求  </span></span><br><span class="line">            reactor.callLater(<span class="number">0</span>,self._next_request) <span class="comment">#事件循环来帮我们调用回调（可设置时间）</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        self.spider = spider</span><br><span class="line">        self._close = defer.Deferred()</span><br><span class="line">        <span class="keyword">yield</span> self._close</span><br><span class="line"></span><br><span class="line">spider = BaiduSpider() <span class="comment"># 创建爬虫对象        </span></span><br><span class="line">        </span><br><span class="line">_active = set()</span><br><span class="line">engine = Engine() <span class="comment"># 创建引擎</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">d = engine.crawl(spider) <span class="comment"># 执行爬取操作</span></span><br><span class="line">_active.add(d)</span><br><span class="line"></span><br><span class="line">dd = defer.DeferredList(_active)</span><br><span class="line">dd.addBoth(<span class="keyword">lambda</span> _:reactor.stop()) <span class="comment"># lambda本身就是个函数</span></span><br><span class="line"></span><br><span class="line">reactor.run()</span><br></pre></td></tr></table></figure>

<p>主要用到的知识点有：</p>
<p><img alt data-src="/2019/11/09/scrapy%E6%A1%86%E6%9E%B6%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/6.jpg" class="lozad"></p>
<p>上面就完成一个Low版的scrapy框架，通过对twisted网络库的使用，能大致理解了scrapy的源码工作原理。但是实在太low了啥东西都写在一块。这里做一个升级版，以解耦和与OOP的方式去编写一个实用的小型的scrapy框架：</p>
<p>创建一个engine.py模块：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor <span class="comment">#事件循环（终止条件，所有的socket都已经移除）</span></span><br><span class="line"><span class="keyword">from</span> twisted.web.client <span class="keyword">import</span> getPage <span class="comment">#socket对象(如果下载完成，自动从事件中移除)</span></span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> defer <span class="comment">#defer.Deferred特殊的socket对象(不会发请求，须手动移除)</span></span><br><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> Queue</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">该自定制框架中，所有的方法名都是参考着scrapy中源码的方法名去命名的，故从中可以剖析整个scrapy框架的源码是怎么去实现的!</span></span><br><span class="line"><span class="string">看此框架代码的时候从最后一个类从下往上看(类中的方法也是如此)，因为这是他的执行顺序！</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Request</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用于封装用户请求信息</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,url,callback)</span>:</span></span><br><span class="line">        self.url = url</span><br><span class="line">        self.callback</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HttpResponse</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用于封装网页下载后传入回调的信息与方法（就是scrapy中的resonpse）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,content,request)</span>:</span></span><br><span class="line">        self.content = content</span><br><span class="line">        self.request = request</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Scheduler</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    任务调度器</span></span><br><span class="line"><span class="string">    可以自定义是内存还是硬盘</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.q = Queue()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_request</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#往外调度器里拿Request</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">        	req = self.q.get(block=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            req = <span class="literal">None</span> <span class="comment">#如果调度器中没东西取了返回None</span></span><br><span class="line">        <span class="keyword">return</span> req</span><br><span class="line">   	</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">enqueue_request</span><span class="params">(self,request)</span>:</span></span><br><span class="line">        <span class="comment">#把Reuqest添加到调度器</span></span><br><span class="line">        self.q.put(request)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#判断队列中的元素个数</span></span><br><span class="line">        <span class="keyword">return</span> self.q.qsize()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExecutionEengine</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    引擎：所有的调度(url,调度器，下载等)由它完成</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._close = <span class="literal">None</span></span><br><span class="line">        self.scheduler = <span class="literal">None</span></span><br><span class="line">        self.max = <span class="number">5</span> <span class="comment">#最大并发数</span></span><br><span class="line">        self.crawlling = [] <span class="comment">#正在爬取得Request列表,最多有最大并发数个</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_response_callback</span><span class="params">(self,content,requset)</span>:</span></span><br><span class="line">        <span class="string">"""封装请求完成后的调调函数，类似一个装饰器对用户的调调函数做修饰"""</span></span><br><span class="line">        self.crawlling.remove(request) <span class="comment">#请求完成，从crawlling中移除</span></span><br><span class="line">        response = HttpResponse(content,request)</span><br><span class="line">        result = requset.callback(response)<span class="comment">#执行用户的回调函数</span></span><br><span class="line">        <span class="keyword">import</span> types</span><br><span class="line">        <span class="keyword">if</span> isinstance(result,types,GeneratorType):</span><br><span class="line">            <span class="comment">#判断返回值是否是一个生成器（req），如果是的话就拿到它然后添加到调度器中</span></span><br><span class="line">            <span class="keyword">for</span> req <span class="keyword">in</span> result:</span><br><span class="line">                self.sheduler.enqueue_request(req) <span class="comment">#放入调度器</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_next_request</span><span class="params">(self)</span>:</span></span><br><span class="line">       <span class="string">"""用来从调度器取Request然后去发送请求"""</span></span><br><span class="line">       	<span class="keyword">if</span> self.scheduler.size() == <span class="number">0</span> <span class="keyword">and</span> len(self.crawlling) == <span class="number">0</span>:</span><br><span class="line">            <span class="comment">#当调度器没有任务且正在爬取的req列表也为空时，手动结束事件循环</span></span><br><span class="line">            self._close.callback(<span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> len(self.crawlling) &lt; self.max:</span><br><span class="line">            req = self.scheduler.next_request() <span class="comment">#从调度器中取到一个request</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> req: <span class="comment">#调度器没有任务就停下</span></span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            self.crawlling.append(req) <span class="comment">#把该request添加到正在爬取的列表中</span></span><br><span class="line">            d = getPage(req.url.encode(<span class="string">'utf-8'</span>)) <span class="comment">#发送请求（注意:url需要是字节类型）</span></span><br><span class="line">            d.addCallback(self.get_response_callback,req)</span><br><span class="line">            <span class="comment">#移除了正在爬取的req后还需要再去取req发请求，所以需要再加一个回调去做爬取操作</span></span><br><span class="line">            d.addCallback(<span class="keyword">lambda</span> _:reactor.callLater(<span class="number">0</span>,self._next_requset))</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @defer.inlineCallbacks</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,start_requests)</span>:</span></span><br><span class="line">        self.scheduler = Scheduler() <span class="comment">#创建调度器对象</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="literal">None</span> <span class="comment">#使用twisted装饰器必须要yield一个返回值，这里只是遵循这个规则所以None！</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">            	req = next(start_requests) <span class="comment">#不断从迭代器拿Request对象</span></span><br><span class="line">            <span class="keyword">except</span> StopIteration <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">break</span> <span class="comment">#拿完会跳出异常故需要异常处理，跳出循环</span></span><br><span class="line">        	self.scheduler.enqueue_request(req) <span class="comment">#把拿到的Request放到调度器中</span></span><br><span class="line">        </span><br><span class="line">        reactor.callLater(<span class="number">0</span>,self._next_request) <span class="comment">#0秒后调用_next_request方法</span></span><br><span class="line">        </span><br><span class="line"><span class="meta">    @defer.inlineCallbacks</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._close = defer.Deferred()</span><br><span class="line">        <span class="keyword">yield</span> self._close</span><br><span class="line">        </span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Crawler</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用于创建引擎和创建spider对象</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_engine</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ExecutionEngine()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_spider</span><span class="params">(self,spider_cls_path)</span>:</span></span><br><span class="line">        <span class="comment">#爬虫路径是这样的字符串：spider.baidu.BaiduSpider，我们要进行分割通过反射导入</span></span><br><span class="line">        module_path,cls_name = spider_cls_path.rsplit(<span class="string">'.'</span>,maxspilit=<span class="number">1</span>) <span class="comment">#右切一个 .</span></span><br><span class="line">    	<span class="keyword">import</span> importlib <span class="comment"># 反射模块</span></span><br><span class="line">        m = importlib.import_module(module_path) <span class="comment">#反射：通过字符串形式导入模块</span></span><br><span class="line">        cls = getattr(m,cls_name)</span><br><span class="line">        <span class="keyword">return</span> cls()</span><br><span class="line">        </span><br><span class="line"><span class="meta">    @defer.inlineCallbacks</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(self,spider_cls_path)</span>:</span></span><br><span class="line">        <span class="string">"""创建引擎和spider对象，并加spider中的req放入调度器中"""</span></span><br><span class="line">        engine = self._create_engine()</span><br><span class="line">        spider = self._create_spider(spider_cls_path)</span><br><span class="line">        <span class="comment">#拿到要发送请求的Request对象</span></span><br><span class="line">        start_requests = iter(spider.start_requests()) <span class="comment">#返回一个生成器，我们转换成迭代器</span></span><br><span class="line">		<span class="comment">#把请求放到调度器去，然后下载页面执回调，这些是引擎的职责所以用engine的方法去做</span></span><br><span class="line">        <span class="keyword">yield</span> engine.open_spider(start_requests) <span class="comment">#将请求放入调度器中</span></span><br><span class="line">        <span class="keyword">yield</span> engine.start() <span class="comment">#相当于yield defer.Deferred()返回Deferred对象</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrawlerProcess</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    开启事件循环</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._active = set()</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(self,spider_cls_path)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        每一个spider都要执行crawl方法，在这里面去进行爬取操作</span></span><br><span class="line"><span class="string">        获取Deferred对象添加到监听终止列表中</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        crawler = Crawler() <span class="comment"># 每个爬虫来都去创建Crawler对象,里面有爬取操作</span></span><br><span class="line">        d = crawler.crawl(spider_cls_path) <span class="comment">#爬取操作，获取Deferred对象</span></span><br><span class="line">    	self._active.add(d) <span class="comment">#添加到终止列表中</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""开启事件循环"""</span></span><br><span class="line">        dd = defer.DeferredList(self._active) <span class="comment">#创建终止列表</span></span><br><span class="line">        <span class="comment">#ps:_active中有几个Deferred对象就代表是有几个爬虫！</span></span><br><span class="line">        dd.addBoth(<span class="keyword">lambda</span> _:reactor.stop())</span><br><span class="line">        reactor.run() <span class="comment">#开启事件循环</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Commond</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定制命令，帮助我们来创建上面的东西</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        crawl_process = CrawlerProcess()</span><br><span class="line">        spider_cls_path_list = [<span class="string">'spider.baidu.BaiduSpider'</span>,] <span class="comment">#爬虫类的路径列表</span></span><br><span class="line">		<span class="keyword">for</span> spider_cls_path <span class="keyword">in</span> spider_cls_path_list:</span><br><span class="line">            crawl_process.crawl(spider_cls_path)<span class="comment">#给每个爬虫创建deferred对象</span></span><br><span class="line">        crawl_process.start() <span class="comment">#就是reactor.run 开启事件循环的一个方法</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    cmd = Commond()</span><br><span class="line">    cmd.run() <span class="comment">#程序运行的入口</span></span><br></pre></td></tr></table></figure>

<p>创建spider目录，在里面放爬虫如baidu.py，用于存放用户创建的爬虫：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> engine <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaiduSpider</span><span class="params">(object)</span>:</span></span><br><span class="line">    name = <span class="string">'baidu'</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        start_url = [<span class="string">'https://www.baidu.com'</span>,]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> start_url:</span><br><span class="line">            <span class="keyword">yield</span> Request(url,self.parse) <span class="comment"># 返回的是一个生成器</span></span><br><span class="line">            </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response,content)</span>:</span></span><br><span class="line">        print(response)</span><br><span class="line">        <span class="comment"># 1.爬取完成后执行回调所以要从crawlling 移除该爬取的url</span></span><br><span class="line">        <span class="comment"># 2.获取parse中yield的返回值</span></span><br><span class="line">        <span class="comment"># 3.再次去队列中获取</span></span><br></pre></td></tr></table></figure>

<p>实际上上面的示例就是scrapy源码的整个过程。上面许多类名和方法名字是参考scrapy源码去命名的。该自定义框架是去模仿scrapy执行流程（从最下面的类开始执行即commod作为入口依次网上调用类与里面的类方法）。通过该示例就可以非常好的来剖析理解scrapy的源码。</p>
<p>理顺了执行流程就可以有思路地去观看剖析源码！基于源码上我们可以做许多自定制的操作！</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;&quot;&quot;以自定制命令为入口，去翻看下面的源码模块&quot;&quot;&quot;</span><br><span class="line">from scrapy.crawler import CrawlerProcess</span><br><span class="line">from scrapy crawler import Crawler</span><br><span class="line">from scrapy.core.engine import ExecutionEngine</span><br></pre></td></tr></table></figure>

<h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>以上这就是我个人对scrapy的进阶学习一些心得，如有误，欢迎指正！</p>
</div></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python%E7%88%AC%E8%99%AB/">python爬虫    </a><a class="post-meta__tags" href="/tags/scrapy/">scrapy    </a><a class="post-meta__tags" href="/tags/%E8%BF%9B%E9%98%B6/">进阶    </a></div><div class="post_share"><div class="social-share" data-image="/img/scrapy入门_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lozad post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/11/16/Tornado%E6%A1%86%E6%9E%B6%E5%85%A5%E9%97%A8/"><img class="prev_cover lozad" data-src="/img/tornado_cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>Tornado框架入门</span></div></a></div><div class="next-post pull-right"><a href="/2019/11/01/Scrapy%E6%A1%86%E6%9E%B6%E4%B9%8B%E4%BB%A3%E7%90%86/"><img class="next_cover lozad" data-src="/img/scrapy入门_cover.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>Scrapy框架之代理与https访问</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/10/30/Scrapy框架/" title="Scrapy框架入门"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">Scrapy框架入门</div></a></div><div class="relatedPosts_item"><a href="/2019/11/01/Scrapy框架之代理/" title="Scrapy框架之代理与https访问"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">Scrapy框架之代理与https访问</div></a></div><div class="relatedPosts_item"><a href="/2019/11/01/Scrapy框架之缓存与中间件/" title="Scrapy框架之缓存与中间件"><img class="relatedPosts_cover lozad"data-src="/img/scrapy入门_cover.jpg"><div class="relatedPosts_title">Scrapy框架之缓存与中间件</div></a></div><div class="relatedPosts_item"><a href="/2019/11/19/Tornado框架进阶/" title="Tornado框架进阶"><img class="relatedPosts_cover lozad"data-src="/img/tornado_cover.jpg"><div class="relatedPosts_title">Tornado框架进阶</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="lv-container" data-id="city" data-uid="MTAyMC80NzI1MC8yMzc1MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div></div><footer style="background-image: url(/img/scrapy入门topimg.jpg)"><div id="footer"><div class="copyright">&copy;2019 - 2020 By Rhubarb C</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换">繁</a><i class="nightshift fa fa-moon-o" id="nightshift" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script color="0,0,255" opacity="0.7" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/js/canvas-nest.js"></script><script src="/js/activate-power-mode.js"></script><script>POWERMODE.colorful = true; // make power mode colorful
POWERMODE.shake = true; // turn off shake
document.body.addEventListener('input', POWERMODE);
</script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();
</script></body></html>